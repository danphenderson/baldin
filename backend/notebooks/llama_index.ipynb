{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama-Index Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APP imports\n",
    "from app import schemas\n",
    "from app.core import db, conf \n",
    "\n",
    "# Third party imports\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.indices.struct_store import JSONQueryEngine\n",
    "\n",
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# ENSURE THAT THE OPENAI_API_KEY IS SET\n",
    "os.environ['OPENAI_API_KEY'] = conf.openai.API_KEY\n",
    "\n",
    "# Define llm\n",
    "llm = OpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a JSONQuery Engine for job leads in the `public/seeds/leads.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': None, 'company': None, 'description': 'RemoteData Engineer12+ MonthsDevelop and maintain SQL scripts, stored procedures, and queries to extract and manipulate\\ndata from various sources, including MSSQL and Postgres databases.Work with MuleSoft (Mule) for data integration and API\\ndevelopment, if required or willing to learn it.Monitor and optimize data pipelines, troubleshoot issues, and ensure\\ndata integrity and performance.Collaborate with data analysts to understand data requirements and support their data\\nanalysis and modeling efforts.Stay up to date with industry trends, best practices, and emerging technologies related to\\ndata engineering, Snowflake, ETL, SQL, and related tools.Solid understanding and hands-on experience with ETL tools and\\nframeworks.Excellent communication and collaboration skills to work effectively with cross-functional teams.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'IT Services and IT Consulting', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-accroid-\\ninc-3778553874?refId=yYkaI0G4nwepPxFJ3klj8A%3D%3D&trackingId=o2sgacdM4tWj%2BuQyJl4lNg%3D%3D&position=17&pageNum=0'}, {'title': None, 'company': None, 'description': 'Role: Data Engineer (DE)Location: Scottsdale AZ (day 1 onsite)Duration: Fulltime Must have skill set: Java, Scala,\\nPython, Spark, S3, Glue, RedshiftYou have 6-8 years of relevant software development experience. You have hands-on\\nexperience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical. Highly analytical and data oriented.\\nExperience in SQL, NoSql Database Data masking of on prem PII data. Develop API calls with using secure data transfer.\\nTake standard output data to lower environments for pre prod testing! Enable secured channels for data models and data\\nscience activities. Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15\\nmins You have experience with development tools and agile methodologies.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'Human Resources Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-de-at-zortech-solutions-\\n3778562998?refId=yYkaI0G4nwepPxFJ3klj8A%3D%3D&trackingId=m5Ttw%2BALz%2FZBFTL2QLNOjw%3D%3D&position=13&pageNum=0'}, {'title': 'Data Engineer', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': 'Global', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': 'Bachelor’s/master’s degree', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=ACNf%2FwdQfBNm%2BGsMg51brQ%3D%3D&trackingId=cdHLS4Mhb2zM5bcKTu3o%2BA%3D%3D&position=25&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Sierra', 'description': \"Data EngineerSierra has been asked by a large risk management and intelligence services firm to identify a Data Engineer\\nto join their staff. You'll play a critical role in designing, building, and maintaining thier dark web and open-source\\ndata repositories, search applications, and APIs. Will also work closely with a cross-functional team to improve and\\nmaintain the reliability, scalability, and performance of these tools and technologies.PAY: 150K plus 10% bonus, great\\nbenefits. No sponsorship available.LOCATION: Hybrid remote/in-office in downtown Chicago (2-3 dayds per week). Chicago\\narea applicants only - no third parties please.ResponsibilitiesDesign and develop scalable search applications to enable\\nefficient data retrieval and indexing from the deep and dark web.Work with internal and external stakeholders to\\noptimize data infrastructure and identify cost savings, where possible.Build and maintain data pipelines to ingest,\\ntransform, and process large volumes of open, deep, and dark web data from diverse sources.Develop and maintain API\\nendpoints for querying dark web data, ensuring efficient and reliable access to our systems.Monitor and optimize search\\nperformance, address bottlenecks, and implement enhancementsImplement data quality and validation measures to ensure\\naccuracy and integrity of indexed data.Identify and integrate optimal database solutions.QualificationsExpertise in data\\nmodeling, infrastructure design, and data integration to enhance thier dataBachelor's or Master's degree in Computer\\nScience, Data Science, or a related field.Proficiency in working with large-scale data processing frameworks, especially\\nElasticsearchSolid understanding of data modeling and schema design principles for efficient search and\\nretrievalFamiliarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or\\nprocessesExperience with API development and management, including authentication, versioning, and performance\\noptimizationProven experience as a Data Engineer, preferably in the development and management of search engines and\\nAPIsDemonstrated knowledge of cloud platforms, especially AWS and AzureExcellent communication and collaboration skills\\nto work effectively in a cross-functional team environmentFamiliarity with building CI/CD pipelines to ensure the\\ndelivery of high-quality softwareKnowledge of data privacy and security considerations when working with sensitive\\ndataStrong programming skills in Python\", 'location': 'Hybrid remote/in-office in downtown Chicago (2-3 days per week)', 'salary': '150K plus 10% bonus', 'job_function': 'Information Technology', 'industries': 'Information Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's or Master's degree in Computer Science, Data Science, or a related field\", 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-chicago-165k-at-sierra-\\nits-3676932344?refId=INeQLMNNzZr3z6agBAxedA%3D%3D&trackingId=%2FY9Cx%2Bx36mWmZ57XKyedAQ%3D%3D&position=10&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Fusemachines', 'description': 'About FusemachinesFusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer\\nMaskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI.\\nWith a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time\\nemployees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.About the\\nrole:This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and\\nmaintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and\\nAdvanced Analytics). Working in the Financial sector and implementing cyber-security use cases.Salary Range:\\nUS$7000/monthJob Type: 1099 ContractStart Date: The contract for this position is scheduled to begin in January\\n2024Qualification / Skill Set Requirement:3+ years of real-world data engineering development experience in Snowflake\\nand AWS (certifications preferred)Proven experience as a Snowflake Developer, with a strong understanding of Snowflake\\narchitecture and concepts.Proficient in snowflake services such as snowpipe, stages, stored procedures, views,\\nmaterialized views, tasks and streams.Must have previous experience working with security datasetsStrong programming\\nskills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and\\nmanipulation.Robust understanding of data partitioning and other optimization techniques in Snowflake.Knowledge of data\\nsecurity measures in Snowflake, including role-based access control (RBAC) and data encryption.Highly skilled in one or\\nmore languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration,\\nstorage, processing and manipulation.Strong knowledge of SDLC tools and technologies, including project management\\nsoftware (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or\\nsimilar) and binary repository manager (AWS CodeArtifact or similar).Skilled in Data Integration from different sources\\nsuch as APIs, databases, flat files, event streaming.Good understanding of Data Modeling and Database Design Principles.\\nBeing able to design and implement efficient database schemas that meet the requirements of the data architecture to\\nsupport data solutions.Strong experience in working with ELT and ETL tools and being able to develop custom integration\\nsolutions as needed.Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and\\nKafka, to be able to handle large volumes of data.Strong experience in designing and implementing Data Warehousing\\nsolutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that\\nextract data from source systems, transform it (DBT), and load it into the data warehouse.Strong experience in\\nOrchestration using Apache Airflow.Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS\\nservices like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etcGood understanding of\\nData Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that\\ndata is accurate, complete, and consistent.Good Problem-Solving skills: being able to troubleshoot data processing\\npipelines and identify performance bottlenecks and other issues.Responsibilities:Follow established design, constructed\\ndata architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination.\\nHandle ELT processes, including data extraction, loading, transformation and load data from various sources into\\nSnowflake.Ensure the reliability, scalability, and efficiency of data systems are maintained at all timesAssist in the\\nconfiguration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior\\nteam members.Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and\\nAnalysts to thoroughly understand data requirements and provide data engineering support.Contribute to data quality\\nassurance efforts, such as implementing data validation checks and tests.Evaluate and implement cutting-edge\\ntechnologies and continue learning and expanding skills in data engineering and cloud platforms.Develop, design, and\\nexecute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance\\nframeworks that align with current analytics demands and industry best practicesDocument data engineering processes and\\ndata flows.Care about architecture, observability, testing, and building reliable infrastructure and data\\npipelines.Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and\\nperformance tuning.Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL\\nqueries and database operations.Assess best practices and design schemas that matches business needs for delivering a\\nmodern analytics solution (descriptive, diagnostic, predictive, prescriptive)Be an active member of our Agile team,\\nparticipating in all ceremonies and continuous improvement activities.Equal Opportunity Employer: Race, Color, Religion,\\nSex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran\\nStatus, or any other legally protected group status.Powered by JazzHRshDrx201iX', 'location': 'Remote', 'salary': 'US$7000/month', 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': 'N/A', 'notes': '', 'hiring_manager': 'N/A', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-fusemachines-\\n3733596267?refId=INeQLMNNzZr3z6agBAxedA%3D%3D&trackingId=QT0wp%2Bv2xp8ufPnqrnn3xQ%3D%3D&position=24&pageNum=0'}, {'title': 'Big Data Engineer', 'company': 'Walgreens', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'United States', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree and at least 2 years of experience in data engineering; OR Graduate Degree in a technical discipline\", 'notes': '', 'hiring_manager': 'Not provided', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=hUqfFzzrGisp%2F7qdM%2BRWqw%3D%3D&trackingId=krs8lYwI97XE0JhC41s94w%3D%3D&position=6&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Dollar General Corporation', 'description': 'Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers\\nSave time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks,\\nhealth and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in\\nconvenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still\\ngrowing. Learn more about Dollar General at www.dollargeneral.com.General Summary Dollar General Corporation has been\\ndelivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by\\noffering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning\\nsupplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations.\\nDollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General\\nat www.dollargeneral.com.Duties & Responsibilities Advanced working SQL knowledge and experience working with relational\\ndatabases, query authoring (SQL) as well as working familiarity with a variety of databases.Assemble large, complex data\\nsets that meet functional / non-functional business requirements.Identify, design, and implement internal process\\nimprovements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability,\\netc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of\\ndata sources using SQL and cloud technologies.Build analytics tools that utilize the data pipelines to provide\\nactionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work\\nwith stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support\\ntheir data infrastructure needs.Knowledge, Skills and Abilities Knowledge of programming languages (e.g. Java and\\nPython)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT,\\nor similar field; a Master’s is a plusData engineering certification (e.g IBM Certified Data Engineer) is a\\nplusExperience with big data tools Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases,\\nincluding Postgres and Cassandra.Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow,\\netc.Experience with Snowflake/Azure cloud services EC2, EMR, RDS, RedshiftExperience with stream-processing systems\\nStorm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages Python, Java, C++,\\nScala, etcWork Experience &/or Education Degree in information technology or computer science with additional vendor-\\nspecific certification.BS or MS degree in Computer Science or a related technical field4+ years of Python or Java\\ndevelopment experience4+ years of SQL experience (No-SQL experience is a plus)4+ years of experience with schema design\\nand dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience\\ndesigning, building, and maintaining data processing systemsExperience working with a cloud platform such as Snowflake /\\nAzure or Databricks#mogul#', 'location': 'N/A', 'salary': 'N/A', 'job_function': 'Information Technology', 'industries': 'Retail', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Degree in Computer Science, IT, or similar field; a Master’s is a plus', 'notes': '', 'hiring_manager': 'N/A', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-dollar-\\ngeneral-3745487485?refId=ld9q3KSO2a4lEIsngwp2vg%3D%3D&trackingId=Abt%2BuR6icV3J7h%2B42Vt6NQ%3D%3D&position=25&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Dollar General Corporation', 'description': 'Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers\\nSave time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks,\\nhealth and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in\\nconvenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still\\ngrowing. Learn more about Dollar General at www.dollargeneral.com.General Summary Dollar General Corporation has been\\ndelivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by\\noffering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning\\nsupplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations.\\nDollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General\\nat www.dollargeneral.com.Duties & Responsibilities Advanced working SQL knowledge and experience working with relational\\ndatabases, query authoring (SQL) as well as working familiarity with a variety of databases.Assemble large, complex data\\nsets that meet functional / non-functional business requirements.Identify, design, and implement internal process\\nimprovements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability,\\netc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of\\ndata sources using SQL and cloud technologies.Build analytics tools that utilize the data pipelines to provide\\nactionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work\\nwith stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support\\ntheir data infrastructure needs.Knowledge, Skills and Abilities Knowledge of programming languages (e.g. Java and\\nPython)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT,\\nor similar field; a Master’s is a plusData engineering certification (e.g IBM Certified Data Engineer) is a\\nplusExperience with big data tools Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases,\\nincluding Postgres and Cassandra.Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow,\\netc.Experience with Snowflake/Azure cloud services EC2, EMR, RDS, RedshiftExperience with stream-processing systems\\nStorm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages Python, Java, C++,\\nScala, etcWork Experience &/or Education Degree in information technology or computer science with additional vendor-\\nspecific certification.BS or MS degree in Computer Science or a related technical field4+ years of Python or Java\\ndevelopment experience4+ years of SQL experience (No-SQL experience is a plus)4+ years of experience with schema design\\nand dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience\\ndesigning, building, and maintaining data processing systemsExperience working with a cloud platform such as Snowflake /\\nAzure or Databricks#mogul#', 'location': 'Unknown', 'salary': 'Unknown', 'job_function': 'Information Technology', 'industries': 'Retail', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Degree in Computer Science, IT, or similar field; a Master’s is a plus', 'notes': '', 'hiring_manager': 'Unknown', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-dollar-\\ngeneral-3745487485?refId=KiOufBohy9Hx7d%2BO7QPHXA%3D%3D&trackingId=ZaHR3aKH%2FaomcLvReZzhyA%3D%3D&position=22&pageNum=0'}, {'title': 'Data Engineer', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': 'Global', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': \"Bachelor's/master's degree\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=5eyAniSzCOFqPd2ejukkvQ%3D%3D&trackingId=FEuQBYlAGtZS6hCxdomrzw%3D%3D&position=24&pageNum=0'}, {'title': 'Data Engineer', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': 'Global', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': 'Bachelor’s/master’s degree', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=Qozz%2FEQHEt%2Bj9JDO9K3q4g%3D%3D&trackingId=G2tiNcWKcF0nwH0F1HKi0A%3D%3D&position=24&pageNum=0'}, {'title': 'Data Engineer I', 'company': 'N/A', 'description': 'Title:Data Engineer I, Req# 27194964Location:Cupertino,CA(Hybrid)Contract: 12+ MonthJob Description Develop data\\nautomation tool for collection, processing, and storing lab data. Set up, maintain, and monitor continuous operation\\nclient devices in labs. Develop test scripts for various client devices. Maintain software revisions through GitHub.\\nBuild ETL for telemetry field dataset and automate data integrity and optimization routines for automatic reporting,\\nanalysis, and error detection. Analyze user and experimental data and use engineering and analytical understanding to\\nresolve battery problems. Provide ad-hoc analysis as necessary.Qualifications 1+ years of experience in software\\nengineering/data science engineering. Proficient in Python for data processing and analysis tool development. Proficient\\nin Linux/Unix (Bash and Shell). Proficient in C for software development Proficient in revision control software such as\\nGitHub. Strong working knowledge in designing, building, and maintaining data ETL pipeline. Experience in SQL.\\nExperience in database modeling and data warehousing principles. Familiarity with job scheduling system. Experience in\\ndata science and analytics, statistical analyses, A/B testing and conducting experiments and investigations in large-\\nscale usage data environment. Self-started with a proven ability to handle multiple tasks with strict deadlines. Proven\\ncreativity to go beyond current tools to deliver best solution to the problem. Outstanding problem solving, critical\\nthinking and interpersonal skills.PayRate: $50-55/hr, W2', 'location': 'Cupertino, CA', 'salary': '$50-55/hr, W2', 'job_function': 'Information Technology', 'industries': 'Staffing and Recruiting', 'employment_type': 'Contract', 'seniority_level': 'Entry level', 'education_level': 'N/A', 'notes': '', 'hiring_manager': 'N/A', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-i-at-\\nwinmax-3759969537?refId=KiOufBohy9Hx7d%2BO7QPHXA%3D%3D&trackingId=Gbk8PvOFOQ38NPx7P5QDeA%3D%3D&position=16&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Sierra', 'description': \"Data EngineerSierra has been asked by a large risk management and intelligence services firm to identify a Data Engineer\\nto join their staff. You'll play a critical role in designing, building, and maintaining thier dark web and open-source\\ndata repositories, search applications, and APIs. Will also work closely with a cross-functional team to improve and\\nmaintain the reliability, scalability, and performance of these tools and technologies.PAY: 150K plus 10% bonus, great\\nbenefits. No sponsorship available.LOCATION: Hybrid remote/in-office in downtown Chicago (2-3 dayds per week). Chicago\\narea applicants only - no third parties please.ResponsibilitiesDesign and develop scalable search applications to enable\\nefficient data retrieval and indexing from the deep and dark web.Work with internal and external stakeholders to\\noptimize data infrastructure and identify cost savings, where possible.Build and maintain data pipelines to ingest,\\ntransform, and process large volumes of open, deep, and dark web data from diverse sources.Develop and maintain API\\nendpoints for querying dark web data, ensuring efficient and reliable access to our systems.Monitor and optimize search\\nperformance, address bottlenecks, and implement enhancementsImplement data quality and validation measures to ensure\\naccuracy and integrity of indexed data.Identify and integrate optimal database solutions.QualificationsExpertise in data\\nmodeling, infrastructure design, and data integration to enhance thier dataBachelor's or Master's degree in Computer\\nScience, Data Science, or a related field.Proficiency in working with large-scale data processing frameworks, especially\\nElasticsearchSolid understanding of data modeling and schema design principles for efficient search and\\nretrievalFamiliarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or\\nprocessesExperience with API development and management, including authentication, versioning, and performance\\noptimizationProven experience as a Data Engineer, preferably in the development and management of search engines and\\nAPIsDemonstrated knowledge of cloud platforms, especially AWS and AzureExcellent communication and collaboration skills\\nto work effectively in a cross-functional team environmentFamiliarity with building CI/CD pipelines to ensure the\\ndelivery of high-quality softwareKnowledge of data privacy and security considerations when working with sensitive\\ndataStrong programming skills in Python\", 'location': 'Hybrid remote/in-office in downtown Chicago (2-3 days per week)', 'salary': '150K plus 10% bonus', 'job_function': 'Information Technology', 'industries': 'Information Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's or Master's degree in Computer Science, Data Science, or a related field\", 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-chicago-165k-at-sierra-\\nits-3676932344?refId=1j12dYfGDaJUMcjoDEzPUg%3D%3D&trackingId=VFj4229rq1e%2FmcyIpQQDqw%3D%3D&position=10&pageNum=0'}, {'title': 'Data Engineer', 'company': 'The Walt Disney Company', 'description': 'About The Role & TeamJoin the Data Engineering team within the Disney Decision Science + Integration (DDSI) organization\\nat The Walt Disney Company. We support clients within Disney Experiences which include Parks & Resorts both domestic and\\ninternational, Consumer Products and Disney Signature Experiences as well as the Disney Entertainment segment which\\ninclude Studios Content (Disney Theatrical Group), General Entertainment Content, and ESPN and Sports Content.We use\\ntechnology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business\\ndecisions and drive business value.As a member of the Data Engineering team you will be responsible for partnering with\\nDecision Science Products, Decision Science, Client and Technology team members on various development and sustainment\\nprojects, ad-hoc requests, prototyping and research initiatives by providing data pipeline and database engineering\\nservices.As a Data Engineer you will work on projects such as Adventures by Disney (AbD), VIP Tours, Resort Inventory\\nOptimization (RIO), and several upcoming initiatives. In this position, the candidate will have tasks to develop,\\nimplement, improve and support our solutions. The work will involve various data engineering activities throughout the\\nproject SDLC.What You Will DoWork assignments may cover activities such as participation in data requirements gathering,\\nsource-to-target mapping, develop and maintaining ELT data pipelines, data quality monitoring, producing input datasets\\nfor science models and visualizations and Batch/Orchestration job scheduling. In addition to technical abilities, the\\nrole is responsible for understanding the business domain and processes, then applying that knowledge to the assigned\\nwork. This role communicates data engineering progress to the project leadership team, and actively participates in\\nmeetings and discussions.Required Qualifications & SkillsMinimum 3 years of related work experienceExperience with\\nELT/ETL data pipeline development and maintenanceExpertise using Python and SQLAbility to showcase an understanding of\\none or more business domainsPrior experience gathering data requirements and producing data design solutionsExperience\\nwith developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code\\ndeployment/promotionExperience crafting and building relational databases (preferably in Postgres or\\nSnowflake)Experience leading and deploying code using a source control product such as GitLab/GitHubAble to formulate\\nsolutions and communicate sophisticated technical concepts to non-technical team membersPreferred\\nQualificationsKnowledgeable with theme park attendance, reservations and/or productsShowed strength interacting with\\nAPI’sExperience with data orchestration tools such as Apache AirflowKnowledgeable on cloud architecture and product\\nofferings, preferably AWSExperience using containerization technologies such as Docker or KubernetesEducationBachelor’s\\ndegree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experienceMaster’s\\ndegree preferred Computer Science, Mathematics, Engineering or related field preferred', 'location': 'Not specified', 'salary': 'Not specified', 'job_function': 'Information Technology', 'industries': 'Entertainment Providers', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's degree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experience. Master's degree preferred.\", 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-the-walt-disney-\\ncompany-3764137624?refId=UQ7Y3xGbf7FjVTCwWTFe5g%3D%3D&trackingId=U9qrAvJ%2Bb2tF9CGPI6f6vA%3D%3D&position=12&pageNum=0'}, {'title': 'Data Engineer', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': 'Global', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': \"Bachelor's/master's degree\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=HSbYYR67Af9HPAndWNdraw%3D%3D&trackingId=RmTEeYHuU9VgJ3o2nwjCzQ%3D%3D&position=24&pageNum=0'}, {'title': 'Data Engineer', 'company': 'REALLY', 'description': \"As a Data Engineer at REALLY, your primary responsibility is to seamlessly integrate data from internal and external\\nsources into a unified warehouse data model. You will play a crucial part in facilitating data and analytics endeavors\\nacross REALLY’s business functions.We’re seeking an individual with hands-on experience in data modeling, test\\nautomation, and the development of efficient data pipelines. You are comfortable with rapid-prototyping, while\\nunderstanding how to build sustainable and scalable solutions. You are eager to understand our business, enabling you to\\nshape the vision of data's impact and value at REALLY.Responsibilities:Create and maintain an internal database for\\ningesting data from various sources into one centralized platform, unlocking the ability for the business to analyze all\\nkey metrics across REALLY’s multiple business functionsThrough your work, the business will be able to analyze the full\\ncustomer journey from first engagement to purchase and track the full lifecycle in one single view of the\\ncustomerPartner with executives and engineering team to bring to life our data philosophy through the right processes\\nand toolsAutomate manual data processes; design and implement internal process improvements for optimizing data delivery\\nand ensuring the highest level of data qualityBecome a subject matter expert on the tools REALLY uses to track data\\nacross the businessAbility to perform root cause analysis on external and internal processes and data to identify\\nopportunities for improvement and answer questionsBuild analytics tools that utilize different data pipeline sources to\\nprovide actionable insights into customer acquisition, behavior (ex. Google Analytics, Search Console, Ads, etc).\\nPartner with our software engineers to develop architectures that enable scalable data extraction and transformation for\\nboth predictive and prescriptive modeling.Qualifications:5+ years of professional (or comparable) data/analytics\\nexperienceExperience with SQL and relational databasesExperience with Data Warehouses such as SnowflakeExperience with\\nBI platforms such as Looker and TableauTechnical expertise in designing and creating data modelsStrong attention to\\ndetailA passion for problem solving with strong analytical capabilities associated with working on unstructured\\ndatasetsA desire to follow exceptional software engineering processes, and familiarity with common engineering process\\ntools like Github and GitlabExcellent written and verbal communication skillsAbility to communicate complex information\\nto non-technical audiencesBonus Experience:Experience with Snowflake, Redshift, or BigQueryProgramming experience in\\nPythonExperience creating data visualizations using JavascriptPerks and Benefits:Competitive compensationMedical,\\ndental, and vision coverage for you and your familyLife insurance paid for by REALLYREALLY is an equal opportunity\\nemployer and we welcome everyone to our team. We believe that diversity is integral to our success, and do not\\ndiscriminate based on race, color, religion, age, or any other basis protected by law.This position is based in Austin,\\nTX.REALLY is an equal opportunity employer and we welcome everyone to our team. We believe that diversity is integral to\\nour success, and do not discriminate based on race, color, religion, age, or any other basis protected by law.Powered by\\nJazzHRO0te1Osi3U\", 'location': 'Austin, TX', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nreally-3771328759?refId=HSbYYR67Af9HPAndWNdraw%3D%3D&trackingId=6SqREIZnMePxvPI9IZ9stg%3D%3D&position=4&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Robinhood', 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': 'Menlo Park, CA; New York, NY; Seattle, WA; Washington, DC', 'salary': '$157000 - $185000', 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-robinhood-\\n3737638667?refId=Qozz%2FEQHEt%2Bj9JDO9K3q4g%3D%3D&trackingId=0zfBeMROwFoqfH7OREbR9w%3D%3D&position=13&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Fusemachines', 'description': 'About FusemachinesFusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer\\nMaskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI.\\nWith a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time\\nemployees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.About the\\nrole:This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and\\nmaintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and\\nAdvanced Analytics). Working in the Financial sector and implementing cyber-security use cases.Salary Range:\\nUS$7000/monthJob Type: 1099 ContractStart Date: The contract for this position is scheduled to begin in January\\n2024Qualification / Skill Set Requirement:3+ years of real-world data engineering development experience in Snowflake\\nand AWS (certifications preferred)Proven experience as a Snowflake Developer, with a strong understanding of Snowflake\\narchitecture and concepts.Proficient in snowflake services such as snowpipe, stages, stored procedures, views,\\nmaterialized views, tasks and streams.Must have previous experience working with security datasetsStrong programming\\nskills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and\\nmanipulation.Robust understanding of data partitioning and other optimization techniques in Snowflake.Knowledge of data\\nsecurity measures in Snowflake, including role-based access control (RBAC) and data encryption.Highly skilled in one or\\nmore languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration,\\nstorage, processing and manipulation.Strong knowledge of SDLC tools and technologies, including project management\\nsoftware (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or\\nsimilar) and binary repository manager (AWS CodeArtifact or similar).Skilled in Data Integration from different sources\\nsuch as APIs, databases, flat files, event streaming.Good understanding of Data Modeling and Database Design Principles.\\nBeing able to design and implement efficient database schemas that meet the requirements of the data architecture to\\nsupport data solutions.Strong experience in working with ELT and ETL tools and being able to develop custom integration\\nsolutions as needed.Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and\\nKafka, to be able to handle large volumes of data.Strong experience in designing and implementing Data Warehousing\\nsolutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that\\nextract data from source systems, transform it (DBT), and load it into the data warehouse.Strong experience in\\nOrchestration using Apache Airflow.Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS\\nservices like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etcGood understanding of\\nData Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that\\ndata is accurate, complete, and consistent.Good Problem-Solving skills: being able to troubleshoot data processing\\npipelines and identify performance bottlenecks and other issues.Responsibilities:Follow established design, constructed\\ndata architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination.\\nHandle ELT processes, including data extraction, loading, transformation and load data from various sources into\\nSnowflake.Ensure the reliability, scalability, and efficiency of data systems are maintained at all timesAssist in the\\nconfiguration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior\\nteam members.Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and\\nAnalysts to thoroughly understand data requirements and provide data engineering support.Contribute to data quality\\nassurance efforts, such as implementing data validation checks and tests.Evaluate and implement cutting-edge\\ntechnologies and continue learning and expanding skills in data engineering and cloud platforms.Develop, design, and\\nexecute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance\\nframeworks that align with current analytics demands and industry best practicesDocument data engineering processes and\\ndata flows.Care about architecture, observability, testing, and building reliable infrastructure and data\\npipelines.Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and\\nperformance tuning.Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL\\nqueries and database operations.Assess best practices and design schemas that matches business needs for delivering a\\nmodern analytics solution (descriptive, diagnostic, predictive, prescriptive)Be an active member of our Agile team,\\nparticipating in all ceremonies and continuous improvement activities.Equal Opportunity Employer: Race, Color, Religion,\\nSex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran\\nStatus, or any other legally protected group status.Powered by JazzHRshDrx201iX', 'location': 'Remote', 'salary': 'US$7000/month', 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': 'Not specified', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-fusemachines-\\n3733596267?refId=m4cyTa0q3I%2F0REHifu8sDQ%3D%3D&trackingId=EWqSoebqXK4C%2ByQw051tjw%3D%3D&position=24&pageNum=0'}, {'title': None, 'company': None, 'description': 'RemoteData Engineer12+ MonthsDevelop and maintain SQL scripts, stored procedures, and queries to extract and manipulate\\ndata from various sources, including MSSQL and Postgres databases.Work with MuleSoft (Mule) for data integration and API\\ndevelopment, if required or willing to learn it.Monitor and optimize data pipelines, troubleshoot issues, and ensure\\ndata integrity and performance.Collaborate with data analysts to understand data requirements and support their data\\nanalysis and modeling efforts.Stay up to date with industry trends, best practices, and emerging technologies related to\\ndata engineering, Snowflake, ETL, SQL, and related tools.Solid understanding and hands-on experience with ETL tools and\\nframeworks.Excellent communication and collaboration skills to work effectively with cross-functional teams.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'IT Services and IT Consulting', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-accroid-\\ninc-3778553874?refId=Qozz%2FEQHEt%2Bj9JDO9K3q4g%3D%3D&trackingId=RzQ%2F91bOrv5InYo1NJcTEQ%3D%3D&position=17&pageNum=0'}, {'title': 'Big Data Engineer', 'company': 'Walgreens Boots Alliance', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'United States', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree and at least 2 years of experience in data engineering; OR Graduate Degree in a technical discipline\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=KiOufBohy9Hx7d%2BO7QPHXA%3D%3D&trackingId=t%2Foor2GMZ6x46ObZTCHr9A%3D%3D&position=6&pageNum=0'}, {'title': 'Big Data Engineer', 'company': 'Walgreens', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'United States', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree or Graduate Degree in a technical discipline\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=bBU5TqxCxwdiQdibF%2FBcSA%3D%3D&trackingId=8a2d3BLr%2FO5CgSTT4Fa2pQ%3D%3D&position=6&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Robinhood', 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': 'Menlo Park, CA; New York, NY; Seattle, WA; Washington, DC', 'salary': '$157000 - $185000', 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Not specified', 'notes': '', 'hiring_manager': 'Not provided', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-robinhood-\\n3737638667?refId=INeQLMNNzZr3z6agBAxedA%3D%3D&trackingId=nl5gyqbd%2BtiNF6Hp%2B3mttQ%3D%3D&position=14&pageNum=0'}, {'title': 'Data Engineer I', 'company': 'Unknown', 'description': 'Title:Data Engineer I, Req# 27194964Location:Cupertino,CA(Hybrid)Contract: 12+ MonthJob Description Develop data\\nautomation tool for collection, processing, and storing lab data. Set up, maintain, and monitor continuous operation\\nclient devices in labs. Develop test scripts for various client devices. Maintain software revisions through GitHub.\\nBuild ETL for telemetry field dataset and automate data integrity and optimization routines for automatic reporting,\\nanalysis, and error detection. Analyze user and experimental data and use engineering and analytical understanding to\\nresolve battery problems. Provide ad-hoc analysis as necessary.Qualifications 1+ years of experience in software\\nengineering/data science engineering. Proficient in Python for data processing and analysis tool development. Proficient\\nin Linux/Unix (Bash and Shell). Proficient in C for software development Proficient in revision control software such as\\nGitHub. Strong working knowledge in designing, building, and maintaining data ETL pipeline. Experience in SQL.\\nExperience in database modeling and data warehousing principles. Familiarity with job scheduling system. Experience in\\ndata science and analytics, statistical analyses, A/B testing and conducting experiments and investigations in large-\\nscale usage data environment. Self-started with a proven ability to handle multiple tasks with strict deadlines. Proven\\ncreativity to go beyond current tools to deliver best solution to the problem. Outstanding problem solving, critical\\nthinking and interpersonal skills.PayRate: $50-55/hr, W2', 'location': 'Cupertino, CA', 'salary': '$50-55/hr, W2', 'job_function': 'Information Technology', 'industries': 'Staffing and Recruiting', 'employment_type': 'Contract', 'seniority_level': 'Entry level', 'education_level': 'Unknown', 'notes': '', 'hiring_manager': 'Unknown', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-i-at-\\nwinmax-3759969537?refId=INeQLMNNzZr3z6agBAxedA%3D%3D&trackingId=yJnNVjBVaBfbbc5TsdaGag%3D%3D&position=16&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Sierra', 'description': \"Data EngineerSierra has been asked by a large risk management and intelligence services firm to identify a Data Engineer\\nto join their staff. You'll play a critical role in designing, building, and maintaining thier dark web and open-source\\ndata repositories, search applications, and APIs. Will also work closely with a cross-functional team to improve and\\nmaintain the reliability, scalability, and performance of these tools and technologies.PAY: 150K plus 10% bonus, great\\nbenefits. No sponsorship available.LOCATION: Hybrid remote/in-office in downtown Chicago (2-3 dayds per week). Chicago\\narea applicants only - no third parties please.ResponsibilitiesDesign and develop scalable search applications to enable\\nefficient data retrieval and indexing from the deep and dark web.Work with internal and external stakeholders to\\noptimize data infrastructure and identify cost savings, where possible.Build and maintain data pipelines to ingest,\\ntransform, and process large volumes of open, deep, and dark web data from diverse sources.Develop and maintain API\\nendpoints for querying dark web data, ensuring efficient and reliable access to our systems.Monitor and optimize search\\nperformance, address bottlenecks, and implement enhancementsImplement data quality and validation measures to ensure\\naccuracy and integrity of indexed data.Identify and integrate optimal database solutions.QualificationsExpertise in data\\nmodeling, infrastructure design, and data integration to enhance thier dataBachelor's or Master's degree in Computer\\nScience, Data Science, or a related field.Proficiency in working with large-scale data processing frameworks, especially\\nElasticsearchSolid understanding of data modeling and schema design principles for efficient search and\\nretrievalFamiliarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or\\nprocessesExperience with API development and management, including authentication, versioning, and performance\\noptimizationProven experience as a Data Engineer, preferably in the development and management of search engines and\\nAPIsDemonstrated knowledge of cloud platforms, especially AWS and AzureExcellent communication and collaboration skills\\nto work effectively in a cross-functional team environmentFamiliarity with building CI/CD pipelines to ensure the\\ndelivery of high-quality softwareKnowledge of data privacy and security considerations when working with sensitive\\ndataStrong programming skills in Python\", 'location': 'Hybrid remote/in-office in downtown Chicago (2-3 days per week)', 'salary': '150K plus 10% bonus', 'job_function': 'Information Technology', 'industries': 'Information Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's or Master's degree in Computer Science, Data Science, or a related field\", 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-chicago-165k-at-sierra-\\nits-3676932344?refId=bBU5TqxCxwdiQdibF%2FBcSA%3D%3D&trackingId=3Yuu%2FurWmEJrE3dF3wR%2FaA%3D%3D&position=9&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Fusemachines', 'description': 'About FusemachinesFusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer\\nMaskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI.\\nWith a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time\\nemployees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.About the\\nrole:This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and\\nmaintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and\\nAdvanced Analytics). Working in the Financial sector and implementing cyber-security use cases.Salary Range:\\nUS$7000/monthJob Type: 1099 ContractStart Date: The contract for this position is scheduled to begin in January\\n2024Qualification / Skill Set Requirement:3+ years of real-world data engineering development experience in Snowflake\\nand AWS (certifications preferred)Proven experience as a Snowflake Developer, with a strong understanding of Snowflake\\narchitecture and concepts.Proficient in snowflake services such as snowpipe, stages, stored procedures, views,\\nmaterialized views, tasks and streams.Must have previous experience working with security datasetsStrong programming\\nskills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and\\nmanipulation.Robust understanding of data partitioning and other optimization techniques in Snowflake.Knowledge of data\\nsecurity measures in Snowflake, including role-based access control (RBAC) and data encryption.Highly skilled in one or\\nmore languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration,\\nstorage, processing and manipulation.Strong knowledge of SDLC tools and technologies, including project management\\nsoftware (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or\\nsimilar) and binary repository manager (AWS CodeArtifact or similar).Skilled in Data Integration from different sources\\nsuch as APIs, databases, flat files, event streaming.Good understanding of Data Modeling and Database Design Principles.\\nBeing able to design and implement efficient database schemas that meet the requirements of the data architecture to\\nsupport data solutions.Strong experience in working with ELT and ETL tools and being able to develop custom integration\\nsolutions as needed.Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and\\nKafka, to be able to handle large volumes of data.Strong experience in designing and implementing Data Warehousing\\nsolutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that\\nextract data from source systems, transform it (DBT), and load it into the data warehouse.Strong experience in\\nOrchestration using Apache Airflow.Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS\\nservices like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etcGood understanding of\\nData Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that\\ndata is accurate, complete, and consistent.Good Problem-Solving skills: being able to troubleshoot data processing\\npipelines and identify performance bottlenecks and other issues.Responsibilities:Follow established design, constructed\\ndata architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination.\\nHandle ELT processes, including data extraction, loading, transformation and load data from various sources into\\nSnowflake.Ensure the reliability, scalability, and efficiency of data systems are maintained at all timesAssist in the\\nconfiguration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior\\nteam members.Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and\\nAnalysts to thoroughly understand data requirements and provide data engineering support.Contribute to data quality\\nassurance efforts, such as implementing data validation checks and tests.Evaluate and implement cutting-edge\\ntechnologies and continue learning and expanding skills in data engineering and cloud platforms.Develop, design, and\\nexecute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance\\nframeworks that align with current analytics demands and industry best practicesDocument data engineering processes and\\ndata flows.Care about architecture, observability, testing, and building reliable infrastructure and data\\npipelines.Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and\\nperformance tuning.Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL\\nqueries and database operations.Assess best practices and design schemas that matches business needs for delivering a\\nmodern analytics solution (descriptive, diagnostic, predictive, prescriptive)Be an active member of our Agile team,\\nparticipating in all ceremonies and continuous improvement activities.Equal Opportunity Employer: Race, Color, Religion,\\nSex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran\\nStatus, or any other legally protected group status.Powered by JazzHRshDrx201iX', 'location': 'Remote', 'salary': 'US$7000/month', 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': 'N/A', 'notes': '', 'hiring_manager': 'Sameer Maskey', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nfusemachines-3733596267?refId=UQ7Y3xGbf7FjVTCwWTFe5g%3D%3D&trackingId=uEHWGb0zOPzlwsRgSoyIAg%3D%3D&position=24&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Dollar General Corporation', 'description': 'Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers\\nSave time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks,\\nhealth and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in\\nconvenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still\\ngrowing. Learn more about Dollar General at www.dollargeneral.com.General Summary Dollar General Corporation has been\\ndelivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by\\noffering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning\\nsupplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations.\\nDollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General\\nat www.dollargeneral.com.Duties & Responsibilities Advanced working SQL knowledge and experience working with relational\\ndatabases, query authoring (SQL) as well as working familiarity with a variety of databases.Assemble large, complex data\\nsets that meet functional / non-functional business requirements.Identify, design, and implement internal process\\nimprovements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability,\\netc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of\\ndata sources using SQL and cloud technologies.Build analytics tools that utilize the data pipelines to provide\\nactionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work\\nwith stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support\\ntheir data infrastructure needs.Knowledge, Skills and Abilities Knowledge of programming languages (e.g. Java and\\nPython)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT,\\nor similar field; a Master’s is a plusData engineering certification (e.g IBM Certified Data Engineer) is a\\nplusExperience with big data tools Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases,\\nincluding Postgres and Cassandra.Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow,\\netc.Experience with Snowflake/Azure cloud services EC2, EMR, RDS, RedshiftExperience with stream-processing systems\\nStorm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages Python, Java, C++,\\nScala, etcWork Experience &/or Education Degree in information technology or computer science with additional vendor-\\nspecific certification.BS or MS degree in Computer Science or a related technical field4+ years of Python or Java\\ndevelopment experience4+ years of SQL experience (No-SQL experience is a plus)4+ years of experience with schema design\\nand dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience\\ndesigning, building, and maintaining data processing systemsExperience working with a cloud platform such as Snowflake /\\nAzure or Databricks#mogul#', 'location': 'Unknown', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Retail', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Degree in Computer Science, IT, or similar field; a Master’s is a plus', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-dollar-general-\\n3745487485?refId=ACNf%2FwdQfBNm%2BGsMg51brQ%3D%3D&trackingId=OySZQasZF%2BpLrTldhOKjnw%3D%3D&position=23&pageNum=0'}, {'title': 'Data Engineer', 'company': 'The Walt Disney Company', 'description': 'About The Role & TeamJoin the Data Engineering team within the Disney Decision Science + Integration (DDSI) organization\\nat The Walt Disney Company. We support clients within Disney Experiences which include Parks & Resorts both domestic and\\ninternational, Consumer Products and Disney Signature Experiences as well as the Disney Entertainment segment which\\ninclude Studios Content (Disney Theatrical Group), General Entertainment Content, and ESPN and Sports Content.We use\\ntechnology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business\\ndecisions and drive business value.As a member of the Data Engineering team you will be responsible for partnering with\\nDecision Science Products, Decision Science, Client and Technology team members on various development and sustainment\\nprojects, ad-hoc requests, prototyping and research initiatives by providing data pipeline and database engineering\\nservices.As a Data Engineer you will work on projects such as Adventures by Disney (AbD), VIP Tours, Resort Inventory\\nOptimization (RIO), and several upcoming initiatives. In this position, the candidate will have tasks to develop,\\nimplement, improve and support our solutions. The work will involve various data engineering activities throughout the\\nproject SDLC.What You Will DoWork assignments may cover activities such as participation in data requirements gathering,\\nsource-to-target mapping, develop and maintaining ELT data pipelines, data quality monitoring, producing input datasets\\nfor science models and visualizations and Batch/Orchestration job scheduling. In addition to technical abilities, the\\nrole is responsible for understanding the business domain and processes, then applying that knowledge to the assigned\\nwork. This role communicates data engineering progress to the project leadership team, and actively participates in\\nmeetings and discussions.Required Qualifications & SkillsMinimum 3 years of related work experienceExperience with\\nELT/ETL data pipeline development and maintenanceExpertise using Python and SQLAbility to showcase an understanding of\\none or more business domainsPrior experience gathering data requirements and producing data design solutionsExperience\\nwith developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code\\ndeployment/promotionExperience crafting and building relational databases (preferably in Postgres or\\nSnowflake)Experience leading and deploying code using a source control product such as GitLab/GitHubAble to formulate\\nsolutions and communicate sophisticated technical concepts to non-technical team membersPreferred\\nQualificationsKnowledgeable with theme park attendance, reservations and/or productsShowed strength interacting with\\nAPI’sExperience with data orchestration tools such as Apache AirflowKnowledgeable on cloud architecture and product\\nofferings, preferably AWSExperience using containerization technologies such as Docker or KubernetesEducationBachelor’s\\ndegree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experienceMaster’s\\ndegree preferred Computer Science, Mathematics, Engineering or related field preferred', 'location': 'Not specified', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Entertainment Providers', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's degree preferred\", 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-the-walt-disney-\\ncompany-3764137624?refId=INeQLMNNzZr3z6agBAxedA%3D%3D&trackingId=P2lFJJpLK8MndDa%2FuNJxqQ%3D%3D&position=13&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Rockstar Games', 'description': 'At Rockstar Games, we create world-class entertainment experiences.A career at Rockstar Games is about being part of a\\nteam working on some of the most creatively rewarding and ambitious projects to be found in any entertainment medium.\\nYou would be welcomed to a dedicated and inclusive environment where you can learn, and collaborate with some of the\\nmost talented people in the industry.Rockstar is seeking a Data Engineer to join a team focused on building a cutting-\\nedge game analytics platform and tools to better understand our players and enhance their experience in our games. This\\nis a full-time permanent position based out of Rockstar’s unique game development studio in Andover, MA.The ideal\\ncandidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability\\nand performance. In collaboration with other data engineers, machine learning engineers, and software engineers, the\\ncandidate will empower the team of analysts and data scientists to deliver data driven insights and applications to\\ncompany stakeholders.What We DoThe Rockstar Analytics team provide insights and actionable results to a wide variety of\\nstakeholders across the organization in support of their decision making.We are currently adding team members to\\nmultiple verticals including; Machine Learning and Game Data Pipeline.ResponsibilitiesImplement and maintain real-time\\nand batch Data Models.Deliver real-time and non-real-time data models to analysts and data scientists who create\\ninsights and analytics applications for our stakeholders.Implement and support streaming technologies such as Kafka,\\nSpark, Cassandra & AzureML.Assist in the development of deployment automation and operational support strategies.Assist\\nin the development of a big data platform in Hadoop using pipeline technologies such as Spark, Airflow, and more to\\nsupport a variety of requirements and applications.Set the standards for warehouse and schema design in massively\\nparallel processing engines such as Hadoop and Snowflake while collaborating with analysts and data scientist in the\\ncreation of efficient data models.Maintain and extend our CI/CD processes and documentation.Qualifications3+ years of\\nwork experience with data modeling, business intelligence and machine learning on big data architectures.2+ years of\\nexperience with the Hadoop ecosystem (HDFS, Spark, Oozie, Impala, etc.) and big data ecosystems (Kafka, Cassandra,\\netc.).2+ years of experience with the Azure ecosystem (Azure ML, Azure Data Factory) Expert in at least one SQL language\\nsuch as T-SQL or PL/SQL.Experience developing and managing data warehouses on a terabyte or petabyte scale.Strong\\nexperience in massively parallel processing & columnar databases.Experience building Real-Time and/or Near-Real-Time ML\\npipelines.Experience with Python, Scala, or Java.Experience with shell scripting.Experience working in a Linux\\nenvironment.SkillsDeep understanding of advanced data warehousing concepts and track record of applying these concepts\\non the job.Ability to manage numerous projects concurrently and strategically, prioritizing when necessary.Good\\ncommunication skills.Dynamic team player.A passion for technology.PLUSESPlease note that these are desirable skills and\\nare not required to apply for the position.Experience with Python based libraries such as Scikit-Learn Experience with\\nDatabricksExperience with Spark-ML, Jupyter Notebook, AzureML.Experience in Lambda architecture.Experience with\\nCI/CD.Familiar with Restful APIs.Experience with Artifact Repositories.Knowledge of the video game industry.How To\\nApplyPlease apply with a resume and cover letter demonstrating how you meet the skills above. If we would like to move\\nforward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through\\nthe process.Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and\\ncompensating employees based on their qualifications and demonstrated ability to perform job responsibilities.If you’ve\\ngot the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates\\nregardless of age, disability, gender identity, sexual orientation, religion, belief, or race.', 'location': 'Andover, MA', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Computer Games', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-rockstar-\\ngames-3719384994?refId=hUqfFzzrGisp%2F7qdM%2BRWqw%3D%3D&trackingId=dem1zuojf%2BMKP3r4k5IuPw%3D%3D&position=24&pageNum=0'}, {'title': 'Software Development and Integration', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': 'Global', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': 'Bachelor’s/master’s degree', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=INeQLMNNzZr3z6agBAxedA%3D%3D&trackingId=7pp01BX0jYguD%2FbeMm4wOA%3D%3D&position=18&pageNum=0'}, {'title': 'Data Engineer (DE)', 'company': '', 'description': 'Role: Data Engineer (DE)Location: Scottsdale AZ (day 1 onsite)Duration: Fulltime Must have skill set: Java, Scala,\\nPython, Spark, S3, Glue, RedshiftYou have 6-8 years of relevant software development experience. You have hands-on\\nexperience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical. Highly analytical and data oriented.\\nExperience in SQL, NoSql Database Data masking of on prem PII data. Develop API calls with using secure data transfer.\\nTake standard output data to lower environments for pre prod testing! Enable secured channels for data models and data\\nscience activities. Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15\\nmins You have experience with development tools and agile methodologies.', 'location': 'Scottsdale AZ', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Human Resources Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-de-at-zortech-solutions-\\n3778562998?refId=5eyAniSzCOFqPd2ejukkvQ%3D%3D&trackingId=U%2BSIJ%2BOjBUWNP1VsxbVk3A%3D%3D&position=13&pageNum=0'}, {'title': None, 'company': None, 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nrobinhood-3737638667?refId=m4cyTa0q3I%2F0REHifu8sDQ%3D%3D&trackingId=MSaQrSX9eYscU4yWysUUhQ%3D%3D&position=13&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Fusemachines', 'description': 'About FusemachinesFusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer\\nMaskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI.\\nWith a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time\\nemployees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.About the\\nrole:This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and\\nmaintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and\\nAdvanced Analytics). Working in the Financial sector and implementing cyber-security use cases.Salary Range:\\nUS$7000/monthJob Type: 1099 ContractStart Date: The contract for this position is scheduled to begin in January\\n2024Qualification / Skill Set Requirement:3+ years of real-world data engineering development experience in Snowflake\\nand AWS (certifications preferred)Proven experience as a Snowflake Developer, with a strong understanding of Snowflake\\narchitecture and concepts.Proficient in snowflake services such as snowpipe, stages, stored procedures, views,\\nmaterialized views, tasks and streams.Must have previous experience working with security datasetsStrong programming\\nskills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and\\nmanipulation.Robust understanding of data partitioning and other optimization techniques in Snowflake.Knowledge of data\\nsecurity measures in Snowflake, including role-based access control (RBAC) and data encryption.Highly skilled in one or\\nmore languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration,\\nstorage, processing and manipulation.Strong knowledge of SDLC tools and technologies, including project management\\nsoftware (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or\\nsimilar) and binary repository manager (AWS CodeArtifact or similar).Skilled in Data Integration from different sources\\nsuch as APIs, databases, flat files, event streaming.Good understanding of Data Modeling and Database Design Principles.\\nBeing able to design and implement efficient database schemas that meet the requirements of the data architecture to\\nsupport data solutions.Strong experience in working with ELT and ETL tools and being able to develop custom integration\\nsolutions as needed.Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and\\nKafka, to be able to handle large volumes of data.Strong experience in designing and implementing Data Warehousing\\nsolutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that\\nextract data from source systems, transform it (DBT), and load it into the data warehouse.Strong experience in\\nOrchestration using Apache Airflow.Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS\\nservices like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etcGood understanding of\\nData Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that\\ndata is accurate, complete, and consistent.Good Problem-Solving skills: being able to troubleshoot data processing\\npipelines and identify performance bottlenecks and other issues.Responsibilities:Follow established design, constructed\\ndata architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination.\\nHandle ELT processes, including data extraction, loading, transformation and load data from various sources into\\nSnowflake.Ensure the reliability, scalability, and efficiency of data systems are maintained at all timesAssist in the\\nconfiguration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior\\nteam members.Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and\\nAnalysts to thoroughly understand data requirements and provide data engineering support.Contribute to data quality\\nassurance efforts, such as implementing data validation checks and tests.Evaluate and implement cutting-edge\\ntechnologies and continue learning and expanding skills in data engineering and cloud platforms.Develop, design, and\\nexecute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance\\nframeworks that align with current analytics demands and industry best practicesDocument data engineering processes and\\ndata flows.Care about architecture, observability, testing, and building reliable infrastructure and data\\npipelines.Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and\\nperformance tuning.Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL\\nqueries and database operations.Assess best practices and design schemas that matches business needs for delivering a\\nmodern analytics solution (descriptive, diagnostic, predictive, prescriptive)Be an active member of our Agile team,\\nparticipating in all ceremonies and continuous improvement activities.Equal Opportunity Employer: Race, Color, Religion,\\nSex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran\\nStatus, or any other legally protected group status.Powered by JazzHRshDrx201iX', 'location': 'Remote', 'salary': 'US$7000/month', 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': '3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)', 'notes': '', 'hiring_manager': 'Sameer Maskey Ph.D.', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nfusemachines-3733596267?refId=ld9q3KSO2a4lEIsngwp2vg%3D%3D&trackingId=yfa4tBjNooeEZzdQx2ASWA%3D%3D&position=24&pageNum=0'}, {'title': None, 'company': None, 'description': 'RemoteData Engineer12+ MonthsDevelop and maintain SQL scripts, stored procedures, and queries to extract and manipulate\\ndata from various sources, including MSSQL and Postgres databases.Work with MuleSoft (Mule) for data integration and API\\ndevelopment, if required or willing to learn it.Monitor and optimize data pipelines, troubleshoot issues, and ensure\\ndata integrity and performance.Collaborate with data analysts to understand data requirements and support their data\\nanalysis and modeling efforts.Stay up to date with industry trends, best practices, and emerging technologies related to\\ndata engineering, Snowflake, ETL, SQL, and related tools.Solid understanding and hands-on experience with ETL tools and\\nframeworks.Excellent communication and collaboration skills to work effectively with cross-functional teams.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'IT Services and IT Consulting', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-accroid-\\ninc-3778553874?refId=m4cyTa0q3I%2F0REHifu8sDQ%3D%3D&trackingId=4Qvt1JCJR6K0SBWp5lfCYQ%3D%3D&position=18&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Robinhood', 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': 'Menlo Park, CA; New York, NY; Seattle, WA; Washington, DC', 'salary': '$157000 - $185000', 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Not specified', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nrobinhood-3737638667?refId=ld9q3KSO2a4lEIsngwp2vg%3D%3D&trackingId=G%2FWtwSC16gFK43IJgJt47Q%3D%3D&position=13&pageNum=0'}, {'title': 'Data Engineer', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': '', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': 'Bachelor’s/master’s degree', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=H4EGkXNa5BoDzVIjuvwgQQ%3D%3D&trackingId=wjV%2B%2BgYZvPhdc%2BaDeXwFag%3D%3D&position=18&pageNum=0'}, {'title': 'Data Engineer', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': 'Global', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': 'Bachelor’s/master’s degree', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=KiOufBohy9Hx7d%2BO7QPHXA%3D%3D&trackingId=v%2BCuFHF8yLGAMHxFoopfmQ%3D%3D&position=24&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Robinhood', 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': 'Menlo Park, CA; New York, NY; Seattle, WA; Washington, DC', 'salary': '$157000 - $185000', 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Not specified', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-robinhood-\\n3737638667?refId=ACNf%2FwdQfBNm%2BGsMg51brQ%3D%3D&trackingId=0vAwwSkHzdotcRAvOZKUPg%3D%3D&position=13&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Fusemachines', 'description': 'About FusemachinesFusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer\\nMaskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI.\\nWith a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time\\nemployees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.About the\\nrole:This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and\\nmaintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and\\nAdvanced Analytics). Working in the Financial sector and implementing cyber-security use cases.Salary Range:\\nUS$7000/monthJob Type: 1099 ContractStart Date: The contract for this position is scheduled to begin in January\\n2024Qualification / Skill Set Requirement:3+ years of real-world data engineering development experience in Snowflake\\nand AWS (certifications preferred)Proven experience as a Snowflake Developer, with a strong understanding of Snowflake\\narchitecture and concepts.Proficient in snowflake services such as snowpipe, stages, stored procedures, views,\\nmaterialized views, tasks and streams.Must have previous experience working with security datasetsStrong programming\\nskills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and\\nmanipulation.Robust understanding of data partitioning and other optimization techniques in Snowflake.Knowledge of data\\nsecurity measures in Snowflake, including role-based access control (RBAC) and data encryption.Highly skilled in one or\\nmore languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration,\\nstorage, processing and manipulation.Strong knowledge of SDLC tools and technologies, including project management\\nsoftware (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or\\nsimilar) and binary repository manager (AWS CodeArtifact or similar).Skilled in Data Integration from different sources\\nsuch as APIs, databases, flat files, event streaming.Good understanding of Data Modeling and Database Design Principles.\\nBeing able to design and implement efficient database schemas that meet the requirements of the data architecture to\\nsupport data solutions.Strong experience in working with ELT and ETL tools and being able to develop custom integration\\nsolutions as needed.Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and\\nKafka, to be able to handle large volumes of data.Strong experience in designing and implementing Data Warehousing\\nsolutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that\\nextract data from source systems, transform it (DBT), and load it into the data warehouse.Strong experience in\\nOrchestration using Apache Airflow.Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS\\nservices like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etcGood understanding of\\nData Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that\\ndata is accurate, complete, and consistent.Good Problem-Solving skills: being able to troubleshoot data processing\\npipelines and identify performance bottlenecks and other issues.Responsibilities:Follow established design, constructed\\ndata architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination.\\nHandle ELT processes, including data extraction, loading, transformation and load data from various sources into\\nSnowflake.Ensure the reliability, scalability, and efficiency of data systems are maintained at all timesAssist in the\\nconfiguration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior\\nteam members.Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and\\nAnalysts to thoroughly understand data requirements and provide data engineering support.Contribute to data quality\\nassurance efforts, such as implementing data validation checks and tests.Evaluate and implement cutting-edge\\ntechnologies and continue learning and expanding skills in data engineering and cloud platforms.Develop, design, and\\nexecute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance\\nframeworks that align with current analytics demands and industry best practicesDocument data engineering processes and\\ndata flows.Care about architecture, observability, testing, and building reliable infrastructure and data\\npipelines.Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and\\nperformance tuning.Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL\\nqueries and database operations.Assess best practices and design schemas that matches business needs for delivering a\\nmodern analytics solution (descriptive, diagnostic, predictive, prescriptive)Be an active member of our Agile team,\\nparticipating in all ceremonies and continuous improvement activities.Equal Opportunity Employer: Race, Color, Religion,\\nSex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran\\nStatus, or any other legally protected group status.Powered by JazzHRshDrx201iX', 'location': 'Remote', 'salary': '$7000/month', 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': 'Not specified', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-fusemachines-\\n3733596267?refId=kYTn7fN4pmu2wMuvUKnT5A%3D%3D&trackingId=R1LUZy6eP09W7N1Hfe%2BiWA%3D%3D&position=24&pageNum=0'}, {'title': None, 'company': None, 'description': 'RemoteData Engineer12+ MonthsDevelop and maintain SQL scripts, stored procedures, and queries to extract and manipulate\\ndata from various sources, including MSSQL and Postgres databases.Work with MuleSoft (Mule) for data integration and API\\ndevelopment, if required or willing to learn it.Monitor and optimize data pipelines, troubleshoot issues, and ensure\\ndata integrity and performance.Collaborate with data analysts to understand data requirements and support their data\\nanalysis and modeling efforts.Stay up to date with industry trends, best practices, and emerging technologies related to\\ndata engineering, Snowflake, ETL, SQL, and related tools.Solid understanding and hands-on experience with ETL tools and\\nframeworks.Excellent communication and collaboration skills to work effectively with cross-functional teams.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'IT Services and IT Consulting', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-accroid-\\ninc-3778553874?refId=H4EGkXNa5BoDzVIjuvwgQQ%3D%3D&trackingId=fzMGMgohhS9IePtOfIcFlg%3D%3D&position=17&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Rockstar Games', 'description': 'At Rockstar Games, we create world-class entertainment experiences.A career at Rockstar Games is about being part of a\\nteam working on some of the most creatively rewarding and ambitious projects to be found in any entertainment medium.\\nYou would be welcomed to a dedicated and inclusive environment where you can learn, and collaborate with some of the\\nmost talented people in the industry.Rockstar is seeking a Data Engineer to join a team focused on building a cutting-\\nedge game analytics platform and tools to better understand our players and enhance their experience in our games. This\\nis a full-time permanent position based out of Rockstar’s unique game development studio in Andover, MA.The ideal\\ncandidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability\\nand performance. In collaboration with other data engineers, machine learning engineers, and software engineers, the\\ncandidate will empower the team of analysts and data scientists to deliver data driven insights and applications to\\ncompany stakeholders.What We DoThe Rockstar Analytics team provide insights and actionable results to a wide variety of\\nstakeholders across the organization in support of their decision making.We are currently adding team members to\\nmultiple verticals including; Machine Learning and Game Data Pipeline.ResponsibilitiesImplement and maintain real-time\\nand batch Data Models.Deliver real-time and non-real-time data models to analysts and data scientists who create\\ninsights and analytics applications for our stakeholders.Implement and support streaming technologies such as Kafka,\\nSpark, Cassandra & AzureML.Assist in the development of deployment automation and operational support strategies.Assist\\nin the development of a big data platform in Hadoop using pipeline technologies such as Spark, Airflow, and more to\\nsupport a variety of requirements and applications.Set the standards for warehouse and schema design in massively\\nparallel processing engines such as Hadoop and Snowflake while collaborating with analysts and data scientist in the\\ncreation of efficient data models.Maintain and extend our CI/CD processes and documentation.Qualifications3+ years of\\nwork experience with data modeling, business intelligence and machine learning on big data architectures.2+ years of\\nexperience with the Hadoop ecosystem (HDFS, Spark, Oozie, Impala, etc.) and big data ecosystems (Kafka, Cassandra,\\netc.).2+ years of experience with the Azure ecosystem (Azure ML, Azure Data Factory) Expert in at least one SQL language\\nsuch as T-SQL or PL/SQL.Experience developing and managing data warehouses on a terabyte or petabyte scale.Strong\\nexperience in massively parallel processing & columnar databases.Experience building Real-Time and/or Near-Real-Time ML\\npipelines.Experience with Python, Scala, or Java.Experience with shell scripting.Experience working in a Linux\\nenvironment.SkillsDeep understanding of advanced data warehousing concepts and track record of applying these concepts\\non the job.Ability to manage numerous projects concurrently and strategically, prioritizing when necessary.Good\\ncommunication skills.Dynamic team player.A passion for technology.PLUSESPlease note that these are desirable skills and\\nare not required to apply for the position.Experience with Python based libraries such as Scikit-Learn Experience with\\nDatabricksExperience with Spark-ML, Jupyter Notebook, AzureML.Experience in Lambda architecture.Experience with\\nCI/CD.Familiar with Restful APIs.Experience with Artifact Repositories.Knowledge of the video game industry.How To\\nApplyPlease apply with a resume and cover letter demonstrating how you meet the skills above. If we would like to move\\nforward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through\\nthe process.Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and\\ncompensating employees based on their qualifications and demonstrated ability to perform job responsibilities.If you’ve\\ngot the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates\\nregardless of age, disability, gender identity, sexual orientation, religion, belief, or race.', 'location': 'Andover, MA', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Computer Games', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-rockstar-\\ngames-3719384994?refId=KiOufBohy9Hx7d%2BO7QPHXA%3D%3D&trackingId=aJv57y4okQ%2Fkfr6UKCivOg%3D%3D&position=23&pageNum=0'}, {'title': 'Data Engineer', 'company': 'The Walt Disney Company', 'description': 'About The Role & TeamJoin the Data Engineering team within the Disney Decision Science + Integration (DDSI) organization\\nat The Walt Disney Company. We support clients within Disney Experiences which include Parks & Resorts both domestic and\\ninternational, Consumer Products and Disney Signature Experiences as well as the Disney Entertainment segment which\\ninclude Studios Content (Disney Theatrical Group), General Entertainment Content, and ESPN and Sports Content.We use\\ntechnology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business\\ndecisions and drive business value.As a member of the Data Engineering team you will be responsible for partnering with\\nDecision Science Products, Decision Science, Client and Technology team members on various development and sustainment\\nprojects, ad-hoc requests, prototyping and research initiatives by providing data pipeline and database engineering\\nservices.As a Data Engineer you will work on projects such as Adventures by Disney (AbD), VIP Tours, Resort Inventory\\nOptimization (RIO), and several upcoming initiatives. In this position, the candidate will have tasks to develop,\\nimplement, improve and support our solutions. The work will involve various data engineering activities throughout the\\nproject SDLC.What You Will DoWork assignments may cover activities such as participation in data requirements gathering,\\nsource-to-target mapping, develop and maintaining ELT data pipelines, data quality monitoring, producing input datasets\\nfor science models and visualizations and Batch/Orchestration job scheduling. In addition to technical abilities, the\\nrole is responsible for understanding the business domain and processes, then applying that knowledge to the assigned\\nwork. This role communicates data engineering progress to the project leadership team, and actively participates in\\nmeetings and discussions.Required Qualifications & SkillsMinimum 3 years of related work experienceExperience with\\nELT/ETL data pipeline development and maintenanceExpertise using Python and SQLAbility to showcase an understanding of\\none or more business domainsPrior experience gathering data requirements and producing data design solutionsExperience\\nwith developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code\\ndeployment/promotionExperience crafting and building relational databases (preferably in Postgres or\\nSnowflake)Experience leading and deploying code using a source control product such as GitLab/GitHubAble to formulate\\nsolutions and communicate sophisticated technical concepts to non-technical team membersPreferred\\nQualificationsKnowledgeable with theme park attendance, reservations and/or productsShowed strength interacting with\\nAPI’sExperience with data orchestration tools such as Apache AirflowKnowledgeable on cloud architecture and product\\nofferings, preferably AWSExperience using containerization technologies such as Docker or KubernetesEducationBachelor’s\\ndegree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experienceMaster’s\\ndegree preferred Computer Science, Mathematics, Engineering or related field preferred', 'location': 'Unknown', 'salary': 'Not specified', 'job_function': 'Information Technology', 'industries': 'Entertainment Providers', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's degree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experience\", 'notes': '', 'hiring_manager': 'Unknown', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-the-walt-disney-\\ncompany-3764137624?refId=5eyAniSzCOFqPd2ejukkvQ%3D%3D&trackingId=ulKZbmnwf6Rt9Y9pFJlFxA%3D%3D&position=16&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Walgreens Boots Alliance, Inc.', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'United States', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree and at least 2 years of experience in data engineering; OR Graduate Degree in a technical discipline\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=UQ7Y3xGbf7FjVTCwWTFe5g%3D%3D&trackingId=pPUuxViSw95v81TFcR%2B4PA%3D%3D&position=7&pageNum=0'}, {'title': None, 'company': None, 'description': 'Role: Data Engineer (DE)Location: Scottsdale AZ (day 1 onsite)Duration: Fulltime Must have skill set: Java, Scala,\\nPython, Spark, S3, Glue, RedshiftYou have 6-8 years of relevant software development experience. You have hands-on\\nexperience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical. Highly analytical and data oriented.\\nExperience in SQL, NoSql Database Data masking of on prem PII data. Develop API calls with using secure data transfer.\\nTake standard output data to lower environments for pre prod testing! Enable secured channels for data models and data\\nscience activities. Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15\\nmins You have experience with development tools and agile methodologies.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'Human Resources Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-de-at-zortech-\\nsolutions-3778562998?refId=UQ7Y3xGbf7FjVTCwWTFe5g%3D%3D&trackingId=cOTTZya5XMAUY8J0DkV%2BUA%3D%3D&position=14&pageNum=0'}, {'title': 'Data Engineer', 'company': 'REALLY', 'description': \"As a Data Engineer at REALLY, your primary responsibility is to seamlessly integrate data from internal and external\\nsources into a unified warehouse data model. You will play a crucial part in facilitating data and analytics endeavors\\nacross REALLY’s business functions.We’re seeking an individual with hands-on experience in data modeling, test\\nautomation, and the development of efficient data pipelines. You are comfortable with rapid-prototyping, while\\nunderstanding how to build sustainable and scalable solutions. You are eager to understand our business, enabling you to\\nshape the vision of data's impact and value at REALLY.Responsibilities:Create and maintain an internal database for\\ningesting data from various sources into one centralized platform, unlocking the ability for the business to analyze all\\nkey metrics across REALLY’s multiple business functionsThrough your work, the business will be able to analyze the full\\ncustomer journey from first engagement to purchase and track the full lifecycle in one single view of the\\ncustomerPartner with executives and engineering team to bring to life our data philosophy through the right processes\\nand toolsAutomate manual data processes; design and implement internal process improvements for optimizing data delivery\\nand ensuring the highest level of data qualityBecome a subject matter expert on the tools REALLY uses to track data\\nacross the businessAbility to perform root cause analysis on external and internal processes and data to identify\\nopportunities for improvement and answer questionsBuild analytics tools that utilize different data pipeline sources to\\nprovide actionable insights into customer acquisition, behavior (ex. Google Analytics, Search Console, Ads, etc).\\nPartner with our software engineers to develop architectures that enable scalable data extraction and transformation for\\nboth predictive and prescriptive modeling.Qualifications:5+ years of professional (or comparable) data/analytics\\nexperienceExperience with SQL and relational databasesExperience with Data Warehouses such as SnowflakeExperience with\\nBI platforms such as Looker and TableauTechnical expertise in designing and creating data modelsStrong attention to\\ndetailA passion for problem solving with strong analytical capabilities associated with working on unstructured\\ndatasetsA desire to follow exceptional software engineering processes, and familiarity with common engineering process\\ntools like Github and GitlabExcellent written and verbal communication skillsAbility to communicate complex information\\nto non-technical audiencesBonus Experience:Experience with Snowflake, Redshift, or BigQueryProgramming experience in\\nPythonExperience creating data visualizations using JavascriptPerks and Benefits:Competitive compensationMedical,\\ndental, and vision coverage for you and your familyLife insurance paid for by REALLYREALLY is an equal opportunity\\nemployer and we welcome everyone to our team. We believe that diversity is integral to our success, and do not\\ndiscriminate based on race, color, religion, age, or any other basis protected by law.This position is based in Austin,\\nTX.REALLY is an equal opportunity employer and we welcome everyone to our team. We believe that diversity is integral to\\nour success, and do not discriminate based on race, color, religion, age, or any other basis protected by law.Powered by\\nJazzHRO0te1Osi3U\", 'location': 'Austin, TX', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nreally-3771328759?refId=b6qNTI3myDV%2FxeyC4CXYqQ%3D%3D&trackingId=9gbVSQcbsNKa0lON2ThBUw%3D%3D&position=4&pageNum=0'}, {'title': 'Data Engineer', 'company': 'REALLY', 'description': \"As a Data Engineer at REALLY, your primary responsibility is to seamlessly integrate data from internal and external\\nsources into a unified warehouse data model. You will play a crucial part in facilitating data and analytics endeavors\\nacross REALLY’s business functions.We’re seeking an individual with hands-on experience in data modeling, test\\nautomation, and the development of efficient data pipelines. You are comfortable with rapid-prototyping, while\\nunderstanding how to build sustainable and scalable solutions. You are eager to understand our business, enabling you to\\nshape the vision of data's impact and value at REALLY.Responsibilities:Create and maintain an internal database for\\ningesting data from various sources into one centralized platform, unlocking the ability for the business to analyze all\\nkey metrics across REALLY’s multiple business functionsThrough your work, the business will be able to analyze the full\\ncustomer journey from first engagement to purchase and track the full lifecycle in one single view of the\\ncustomerPartner with executives and engineering team to bring to life our data philosophy through the right processes\\nand toolsAutomate manual data processes; design and implement internal process improvements for optimizing data delivery\\nand ensuring the highest level of data qualityBecome a subject matter expert on the tools REALLY uses to track data\\nacross the businessAbility to perform root cause analysis on external and internal processes and data to identify\\nopportunities for improvement and answer questionsBuild analytics tools that utilize different data pipeline sources to\\nprovide actionable insights into customer acquisition, behavior (ex. Google Analytics, Search Console, Ads, etc).\\nPartner with our software engineers to develop architectures that enable scalable data extraction and transformation for\\nboth predictive and prescriptive modeling.Qualifications:5+ years of professional (or comparable) data/analytics\\nexperienceExperience with SQL and relational databasesExperience with Data Warehouses such as SnowflakeExperience with\\nBI platforms such as Looker and TableauTechnical expertise in designing and creating data modelsStrong attention to\\ndetailA passion for problem solving with strong analytical capabilities associated with working on unstructured\\ndatasetsA desire to follow exceptional software engineering processes, and familiarity with common engineering process\\ntools like Github and GitlabExcellent written and verbal communication skillsAbility to communicate complex information\\nto non-technical audiencesBonus Experience:Experience with Snowflake, Redshift, or BigQueryProgramming experience in\\nPythonExperience creating data visualizations using JavascriptPerks and Benefits:Competitive compensationMedical,\\ndental, and vision coverage for you and your familyLife insurance paid for by REALLYREALLY is an equal opportunity\\nemployer and we welcome everyone to our team. We believe that diversity is integral to our success, and do not\\ndiscriminate based on race, color, religion, age, or any other basis protected by law.This position is based in Austin,\\nTX.REALLY is an equal opportunity employer and we welcome everyone to our team. We believe that diversity is integral to\\nour success, and do not discriminate based on race, color, religion, age, or any other basis protected by law.Powered by\\nJazzHRO0te1Osi3U\", 'location': 'Austin, TX', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nreally-3771328759?refId=5eyAniSzCOFqPd2ejukkvQ%3D%3D&trackingId=0PYXSXYRSy2ek1igQGN0VQ%3D%3D&position=4&pageNum=0'}, {'title': 'Data Engineer', 'company': 'The Walt Disney Company', 'description': 'About The Role & TeamJoin the Data Engineering team within the Disney Decision Science + Integration (DDSI) organization\\nat The Walt Disney Company. We support clients within Disney Experiences which include Parks & Resorts both domestic and\\ninternational, Consumer Products and Disney Signature Experiences as well as the Disney Entertainment segment which\\ninclude Studios Content (Disney Theatrical Group), General Entertainment Content, and ESPN and Sports Content.We use\\ntechnology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business\\ndecisions and drive business value.As a member of the Data Engineering team you will be responsible for partnering with\\nDecision Science Products, Decision Science, Client and Technology team members on various development and sustainment\\nprojects, ad-hoc requests, prototyping and research initiatives by providing data pipeline and database engineering\\nservices.As a Data Engineer you will work on projects such as Adventures by Disney (AbD), VIP Tours, Resort Inventory\\nOptimization (RIO), and several upcoming initiatives. In this position, the candidate will have tasks to develop,\\nimplement, improve and support our solutions. The work will involve various data engineering activities throughout the\\nproject SDLC.What You Will DoWork assignments may cover activities such as participation in data requirements gathering,\\nsource-to-target mapping, develop and maintaining ELT data pipelines, data quality monitoring, producing input datasets\\nfor science models and visualizations and Batch/Orchestration job scheduling. In addition to technical abilities, the\\nrole is responsible for understanding the business domain and processes, then applying that knowledge to the assigned\\nwork. This role communicates data engineering progress to the project leadership team, and actively participates in\\nmeetings and discussions.Required Qualifications & SkillsMinimum 3 years of related work experienceExperience with\\nELT/ETL data pipeline development and maintenanceExpertise using Python and SQLAbility to showcase an understanding of\\none or more business domainsPrior experience gathering data requirements and producing data design solutionsExperience\\nwith developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code\\ndeployment/promotionExperience crafting and building relational databases (preferably in Postgres or\\nSnowflake)Experience leading and deploying code using a source control product such as GitLab/GitHubAble to formulate\\nsolutions and communicate sophisticated technical concepts to non-technical team membersPreferred\\nQualificationsKnowledgeable with theme park attendance, reservations and/or productsShowed strength interacting with\\nAPI’sExperience with data orchestration tools such as Apache AirflowKnowledgeable on cloud architecture and product\\nofferings, preferably AWSExperience using containerization technologies such as Docker or KubernetesEducationBachelor’s\\ndegree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experienceMaster’s\\ndegree preferred Computer Science, Mathematics, Engineering or related field preferred', 'location': 'Not specified', 'salary': 'Not specified', 'job_function': 'Information Technology', 'industries': 'Entertainment Providers', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's degree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experience. Master's degree preferred Computer Science, Mathematics, Engineering or related field preferred\", 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-the-walt-disney-\\ncompany-3764137624?refId=H4EGkXNa5BoDzVIjuvwgQQ%3D%3D&trackingId=fyUKmo3P%2Far8Ys9IyyqxyQ%3D%3D&position=12&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Dollar General Corporation', 'description': 'Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers\\nSave time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks,\\nhealth and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in\\nconvenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still\\ngrowing. Learn more about Dollar General at www.dollargeneral.com.General Summary Dollar General Corporation has been\\ndelivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by\\noffering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning\\nsupplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations.\\nDollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General\\nat www.dollargeneral.com.Duties & Responsibilities Advanced working SQL knowledge and experience working with relational\\ndatabases, query authoring (SQL) as well as working familiarity with a variety of databases.Assemble large, complex data\\nsets that meet functional / non-functional business requirements.Identify, design, and implement internal process\\nimprovements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability,\\netc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of\\ndata sources using SQL and cloud technologies.Build analytics tools that utilize the data pipelines to provide\\nactionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work\\nwith stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support\\ntheir data infrastructure needs.Knowledge, Skills and Abilities Knowledge of programming languages (e.g. Java and\\nPython)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT,\\nor similar field; a Master’s is a plusData engineering certification (e.g IBM Certified Data Engineer) is a\\nplusExperience with big data tools Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases,\\nincluding Postgres and Cassandra.Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow,\\netc.Experience with Snowflake/Azure cloud services EC2, EMR, RDS, RedshiftExperience with stream-processing systems\\nStorm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages Python, Java, C++,\\nScala, etcWork Experience &/or Education Degree in information technology or computer science with additional vendor-\\nspecific certification.BS or MS degree in Computer Science or a related technical field4+ years of Python or Java\\ndevelopment experience4+ years of SQL experience (No-SQL experience is a plus)4+ years of experience with schema design\\nand dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience\\ndesigning, building, and maintaining data processing systemsExperience working with a cloud platform such as Snowflake /\\nAzure or Databricks#mogul#', 'location': 'Unknown', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Retail', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Degree in Computer Science, IT, or similar field; a Master’s is a plus', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-dollar-\\ngeneral-3745487485?refId=m4cyTa0q3I%2F0REHifu8sDQ%3D%3D&trackingId=Cj%2FzdtC1L07RWgxFZ9ykLg%3D%3D&position=25&pageNum=0'}, {'title': None, 'company': None, 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-robinhood-\\n3737638667?refId=bBU5TqxCxwdiQdibF%2FBcSA%3D%3D&trackingId=EmU628%2BkX1W1D7%2BAfk2iBQ%3D%3D&position=12&pageNum=0'}, {'title': None, 'company': None, 'description': 'Role: Data Engineer (DE)Location: Scottsdale AZ (day 1 onsite)Duration: Fulltime Must have skill set: Java, Scala,\\nPython, Spark, S3, Glue, RedshiftYou have 6-8 years of relevant software development experience. You have hands-on\\nexperience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical. Highly analytical and data oriented.\\nExperience in SQL, NoSql Database Data masking of on prem PII data. Develop API calls with using secure data transfer.\\nTake standard output data to lower environments for pre prod testing! Enable secured channels for data models and data\\nscience activities. Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15\\nmins You have experience with development tools and agile methodologies.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'Human Resources Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-de-at-zortech-\\nsolutions-3778562998?refId=KiOufBohy9Hx7d%2BO7QPHXA%3D%3D&trackingId=TquPEs8zhIG1V1kl1c9XcA%3D%3D&position=13&pageNum=0'}, {'title': 'Data Engineer', 'company': 'The Walt Disney Company', 'description': 'About The Role & TeamJoin the Data Engineering team within the Disney Decision Science + Integration (DDSI) organization\\nat The Walt Disney Company. We support clients within Disney Experiences which include Parks & Resorts both domestic and\\ninternational, Consumer Products and Disney Signature Experiences as well as the Disney Entertainment segment which\\ninclude Studios Content (Disney Theatrical Group), General Entertainment Content, and ESPN and Sports Content.We use\\ntechnology, data analytics, optimization, statistical and econometric modeling to explore opportunities, shape business\\ndecisions and drive business value.As a member of the Data Engineering team you will be responsible for partnering with\\nDecision Science Products, Decision Science, Client and Technology team members on various development and sustainment\\nprojects, ad-hoc requests, prototyping and research initiatives by providing data pipeline and database engineering\\nservices.As a Data Engineer you will work on projects such as Adventures by Disney (AbD), VIP Tours, Resort Inventory\\nOptimization (RIO), and several upcoming initiatives. In this position, the candidate will have tasks to develop,\\nimplement, improve and support our solutions. The work will involve various data engineering activities throughout the\\nproject SDLC.What You Will DoWork assignments may cover activities such as participation in data requirements gathering,\\nsource-to-target mapping, develop and maintaining ELT data pipelines, data quality monitoring, producing input datasets\\nfor science models and visualizations and Batch/Orchestration job scheduling. In addition to technical abilities, the\\nrole is responsible for understanding the business domain and processes, then applying that knowledge to the assigned\\nwork. This role communicates data engineering progress to the project leadership team, and actively participates in\\nmeetings and discussions.Required Qualifications & SkillsMinimum 3 years of related work experienceExperience with\\nELT/ETL data pipeline development and maintenanceExpertise using Python and SQLAbility to showcase an understanding of\\none or more business domainsPrior experience gathering data requirements and producing data design solutionsExperience\\nwith developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code\\ndeployment/promotionExperience crafting and building relational databases (preferably in Postgres or\\nSnowflake)Experience leading and deploying code using a source control product such as GitLab/GitHubAble to formulate\\nsolutions and communicate sophisticated technical concepts to non-technical team membersPreferred\\nQualificationsKnowledgeable with theme park attendance, reservations and/or productsShowed strength interacting with\\nAPI’sExperience with data orchestration tools such as Apache AirflowKnowledgeable on cloud architecture and product\\nofferings, preferably AWSExperience using containerization technologies such as Docker or KubernetesEducationBachelor’s\\ndegree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experienceMaster’s\\ndegree preferred Computer Science, Mathematics, Engineering or related field preferred', 'location': 'Not specified', 'salary': 'Not specified', 'job_function': 'Information Technology', 'industries': 'Entertainment Providers', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': 'Bachelor’s degree in Computer Science, Mathematics, Engineering or related field preferred/or equivalent work experience', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-the-walt-disney-\\ncompany-3764137624?refId=kYTn7fN4pmu2wMuvUKnT5A%3D%3D&trackingId=VguVaswmI6u%2BZqTm2nY9Rg%3D%3D&position=12&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Fusemachines', 'description': 'About FusemachinesFusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer\\nMaskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI.\\nWith a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time\\nemployees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.About the\\nrole:This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and\\nmaintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and\\nAdvanced Analytics). Working in the Financial sector and implementing cyber-security use cases.Salary Range:\\nUS$7000/monthJob Type: 1099 ContractStart Date: The contract for this position is scheduled to begin in January\\n2024Qualification / Skill Set Requirement:3+ years of real-world data engineering development experience in Snowflake\\nand AWS (certifications preferred)Proven experience as a Snowflake Developer, with a strong understanding of Snowflake\\narchitecture and concepts.Proficient in snowflake services such as snowpipe, stages, stored procedures, views,\\nmaterialized views, tasks and streams.Must have previous experience working with security datasetsStrong programming\\nskills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and\\nmanipulation.Robust understanding of data partitioning and other optimization techniques in Snowflake.Knowledge of data\\nsecurity measures in Snowflake, including role-based access control (RBAC) and data encryption.Highly skilled in one or\\nmore languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration,\\nstorage, processing and manipulation.Strong knowledge of SDLC tools and technologies, including project management\\nsoftware (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or\\nsimilar) and binary repository manager (AWS CodeArtifact or similar).Skilled in Data Integration from different sources\\nsuch as APIs, databases, flat files, event streaming.Good understanding of Data Modeling and Database Design Principles.\\nBeing able to design and implement efficient database schemas that meet the requirements of the data architecture to\\nsupport data solutions.Strong experience in working with ELT and ETL tools and being able to develop custom integration\\nsolutions as needed.Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and\\nKafka, to be able to handle large volumes of data.Strong experience in designing and implementing Data Warehousing\\nsolutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that\\nextract data from source systems, transform it (DBT), and load it into the data warehouse.Strong experience in\\nOrchestration using Apache Airflow.Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS\\nservices like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etcGood understanding of\\nData Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that\\ndata is accurate, complete, and consistent.Good Problem-Solving skills: being able to troubleshoot data processing\\npipelines and identify performance bottlenecks and other issues.Responsibilities:Follow established design, constructed\\ndata architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination.\\nHandle ELT processes, including data extraction, loading, transformation and load data from various sources into\\nSnowflake.Ensure the reliability, scalability, and efficiency of data systems are maintained at all timesAssist in the\\nconfiguration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior\\nteam members.Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and\\nAnalysts to thoroughly understand data requirements and provide data engineering support.Contribute to data quality\\nassurance efforts, such as implementing data validation checks and tests.Evaluate and implement cutting-edge\\ntechnologies and continue learning and expanding skills in data engineering and cloud platforms.Develop, design, and\\nexecute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance\\nframeworks that align with current analytics demands and industry best practicesDocument data engineering processes and\\ndata flows.Care about architecture, observability, testing, and building reliable infrastructure and data\\npipelines.Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and\\nperformance tuning.Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL\\nqueries and database operations.Assess best practices and design schemas that matches business needs for delivering a\\nmodern analytics solution (descriptive, diagnostic, predictive, prescriptive)Be an active member of our Agile team,\\nparticipating in all ceremonies and continuous improvement activities.Equal Opportunity Employer: Race, Color, Religion,\\nSex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran\\nStatus, or any other legally protected group status.Powered by JazzHRshDrx201iX', 'location': 'Remote', 'salary': 'US$7000/month', 'job_function': 'Information Technology', 'industries': 'Internet Publishing', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's degree or higher\", 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nfusemachines-3733596267?refId=H4EGkXNa5BoDzVIjuvwgQQ%3D%3D&trackingId=jdsRPwewKyf6qO0kdmsIyA%3D%3D&position=24&pageNum=0'}, {'title': 'Big Data Engineer', 'company': 'Walgreens Boots Alliance', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'United States', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree or Graduate Degree\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=kYTn7fN4pmu2wMuvUKnT5A%3D%3D&trackingId=btU3t4yNj0B8Zkzff%2BRRGw%3D%3D&position=7&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Dollar General Corporation', 'description': 'Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers\\nSave time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks,\\nhealth and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in\\nconvenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still\\ngrowing. Learn more about Dollar General at www.dollargeneral.com.General Summary Dollar General Corporation has been\\ndelivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by\\noffering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning\\nsupplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations.\\nDollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General\\nat www.dollargeneral.com.Duties & Responsibilities Advanced working SQL knowledge and experience working with relational\\ndatabases, query authoring (SQL) as well as working familiarity with a variety of databases.Assemble large, complex data\\nsets that meet functional / non-functional business requirements.Identify, design, and implement internal process\\nimprovements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability,\\netc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of\\ndata sources using SQL and cloud technologies.Build analytics tools that utilize the data pipelines to provide\\nactionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work\\nwith stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support\\ntheir data infrastructure needs.Knowledge, Skills and Abilities Knowledge of programming languages (e.g. Java and\\nPython)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT,\\nor similar field; a Master’s is a plusData engineering certification (e.g IBM Certified Data Engineer) is a\\nplusExperience with big data tools Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases,\\nincluding Postgres and Cassandra.Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow,\\netc.Experience with Snowflake/Azure cloud services EC2, EMR, RDS, RedshiftExperience with stream-processing systems\\nStorm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages Python, Java, C++,\\nScala, etcWork Experience &/or Education Degree in information technology or computer science with additional vendor-\\nspecific certification.BS or MS degree in Computer Science or a related technical field4+ years of Python or Java\\ndevelopment experience4+ years of SQL experience (No-SQL experience is a plus)4+ years of experience with schema design\\nand dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience\\ndesigning, building, and maintaining data processing systemsExperience working with a cloud platform such as Snowflake /\\nAzure or Databricks#mogul#', 'location': 'Multiple locations', 'salary': 'Not specified', 'job_function': 'Information Technology', 'industries': 'Retail', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Degree in Computer Science, IT, or similar field; a Master’s is a plus', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-dollar-\\ngeneral-3745487485?refId=HSbYYR67Af9HPAndWNdraw%3D%3D&trackingId=8cUSpUH4%2FQQ%2Bepd1eTKbmA%3D%3D&position=22&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Robinhood', 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': 'Menlo Park, CA; New York, NY; Seattle, WA; Washington, DC', 'salary': '$157000 - $185000 (US Zone 1)', 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Not specified', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nrobinhood-3737638667?refId=yYkaI0G4nwepPxFJ3klj8A%3D%3D&trackingId=DrjxJR14LDNCVyIpeek8Ew%3D%3D&position=12&pageNum=0'}, {'title': None, 'company': None, 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': None, 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=hUqfFzzrGisp%2F7qdM%2BRWqw%3D%3D&trackingId=D492S2sfIwf%2BFv5HaIH9iw%3D%3D&position=25&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Rockstar Games', 'description': 'At Rockstar Games, we create world-class entertainment experiences.A career at Rockstar Games is about being part of a\\nteam working on some of the most creatively rewarding and ambitious projects to be found in any entertainment medium.\\nYou would be welcomed to a dedicated and inclusive environment where you can learn, and collaborate with some of the\\nmost talented people in the industry.Rockstar is seeking a Data Engineer to join a team focused on building a cutting-\\nedge game analytics platform and tools to better understand our players and enhance their experience in our games. This\\nis a full-time permanent position based out of Rockstar’s unique game development studio in Andover, MA.The ideal\\ncandidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability\\nand performance. In collaboration with other data engineers, machine learning engineers, and software engineers, the\\ncandidate will empower the team of analysts and data scientists to deliver data driven insights and applications to\\ncompany stakeholders.What We DoThe Rockstar Analytics team provide insights and actionable results to a wide variety of\\nstakeholders across the organization in support of their decision making.We are currently adding team members to\\nmultiple verticals including; Machine Learning and Game Data Pipeline.ResponsibilitiesImplement and maintain real-time\\nand batch Data Models.Deliver real-time and non-real-time data models to analysts and data scientists who create\\ninsights and analytics applications for our stakeholders.Implement and support streaming technologies such as Kafka,\\nSpark, Cassandra & AzureML.Assist in the development of deployment automation and operational support strategies.Assist\\nin the development of a big data platform in Hadoop using pipeline technologies such as Spark, Airflow, and more to\\nsupport a variety of requirements and applications.Set the standards for warehouse and schema design in massively\\nparallel processing engines such as Hadoop and Snowflake while collaborating with analysts and data scientist in the\\ncreation of efficient data models.Maintain and extend our CI/CD processes and documentation.Qualifications3+ years of\\nwork experience with data modeling, business intelligence and machine learning on big data architectures.2+ years of\\nexperience with the Hadoop ecosystem (HDFS, Spark, Oozie, Impala, etc.) and big data ecosystems (Kafka, Cassandra,\\netc.).2+ years of experience with the Azure ecosystem (Azure ML, Azure Data Factory) Expert in at least one SQL language\\nsuch as T-SQL or PL/SQL.Experience developing and managing data warehouses on a terabyte or petabyte scale.Strong\\nexperience in massively parallel processing & columnar databases.Experience building Real-Time and/or Near-Real-Time ML\\npipelines.Experience with Python, Scala, or Java.Experience with shell scripting.Experience working in a Linux\\nenvironment.SkillsDeep understanding of advanced data warehousing concepts and track record of applying these concepts\\non the job.Ability to manage numerous projects concurrently and strategically, prioritizing when necessary.Good\\ncommunication skills.Dynamic team player.A passion for technology.PLUSESPlease note that these are desirable skills and\\nare not required to apply for the position.Experience with Python based libraries such as Scikit-Learn Experience with\\nDatabricksExperience with Spark-ML, Jupyter Notebook, AzureML.Experience in Lambda architecture.Experience with\\nCI/CD.Familiar with Restful APIs.Experience with Artifact Repositories.Knowledge of the video game industry.How To\\nApplyPlease apply with a resume and cover letter demonstrating how you meet the skills above. If we would like to move\\nforward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through\\nthe process.Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and\\ncompensating employees based on their qualifications and demonstrated ability to perform job responsibilities.If you’ve\\ngot the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates\\nregardless of age, disability, gender identity, sexual orientation, religion, belief, or race.', 'location': 'Andover, MA', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Computer Games', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-rockstar-\\ngames-3719384994?refId=Qozz%2FEQHEt%2Bj9JDO9K3q4g%3D%3D&trackingId=8MfWTydoSdyfsJ8%2BswgGnw%3D%3D&position=23&pageNum=0'}, {'title': 'Data Engineer', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': 'Global', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': \"Bachelor's/master's degree\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=ld9q3KSO2a4lEIsngwp2vg%3D%3D&trackingId=ojBezkTsmyIdCf6gozJH2g%3D%3D&position=18&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Rockstar Games', 'description': 'At Rockstar Games, we create world-class entertainment experiences.A career at Rockstar Games is about being part of a\\nteam working on some of the most creatively rewarding and ambitious projects to be found in any entertainment medium.\\nYou would be welcomed to a dedicated and inclusive environment where you can learn, and collaborate with some of the\\nmost talented people in the industry.Rockstar is seeking a Data Engineer to join a team focused on building a cutting-\\nedge game analytics platform and tools to better understand our players and enhance their experience in our games. This\\nis a full-time permanent position based out of Rockstar’s unique game development studio in Andover, MA.The ideal\\ncandidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability\\nand performance. In collaboration with other data engineers, machine learning engineers, and software engineers, the\\ncandidate will empower the team of analysts and data scientists to deliver data driven insights and applications to\\ncompany stakeholders.What We DoThe Rockstar Analytics team provide insights and actionable results to a wide variety of\\nstakeholders across the organization in support of their decision making.We are currently adding team members to\\nmultiple verticals including; Machine Learning and Game Data Pipeline.ResponsibilitiesImplement and maintain real-time\\nand batch Data Models.Deliver real-time and non-real-time data models to analysts and data scientists who create\\ninsights and analytics applications for our stakeholders.Implement and support streaming technologies such as Kafka,\\nSpark, Cassandra & AzureML.Assist in the development of deployment automation and operational support strategies.Assist\\nin the development of a big data platform in Hadoop using pipeline technologies such as Spark, Airflow, and more to\\nsupport a variety of requirements and applications.Set the standards for warehouse and schema design in massively\\nparallel processing engines such as Hadoop and Snowflake while collaborating with analysts and data scientist in the\\ncreation of efficient data models.Maintain and extend our CI/CD processes and documentation.Qualifications3+ years of\\nwork experience with data modeling, business intelligence and machine learning on big data architectures.2+ years of\\nexperience with the Hadoop ecosystem (HDFS, Spark, Oozie, Impala, etc.) and big data ecosystems (Kafka, Cassandra,\\netc.).2+ years of experience with the Azure ecosystem (Azure ML, Azure Data Factory) Expert in at least one SQL language\\nsuch as T-SQL or PL/SQL.Experience developing and managing data warehouses on a terabyte or petabyte scale.Strong\\nexperience in massively parallel processing & columnar databases.Experience building Real-Time and/or Near-Real-Time ML\\npipelines.Experience with Python, Scala, or Java.Experience with shell scripting.Experience working in a Linux\\nenvironment.SkillsDeep understanding of advanced data warehousing concepts and track record of applying these concepts\\non the job.Ability to manage numerous projects concurrently and strategically, prioritizing when necessary.Good\\ncommunication skills.Dynamic team player.A passion for technology.PLUSESPlease note that these are desirable skills and\\nare not required to apply for the position.Experience with Python based libraries such as Scikit-Learn Experience with\\nDatabricksExperience with Spark-ML, Jupyter Notebook, AzureML.Experience in Lambda architecture.Experience with\\nCI/CD.Familiar with Restful APIs.Experience with Artifact Repositories.Knowledge of the video game industry.How To\\nApplyPlease apply with a resume and cover letter demonstrating how you meet the skills above. If we would like to move\\nforward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through\\nthe process.Rockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and\\ncompensating employees based on their qualifications and demonstrated ability to perform job responsibilities.If you’ve\\ngot the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates\\nregardless of age, disability, gender identity, sexual orientation, religion, belief, or race.', 'location': 'Andover, MA', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Computer Games', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-rockstar-\\ngames-3719384994?refId=yYkaI0G4nwepPxFJ3klj8A%3D%3D&trackingId=Y5DeKd40cjMWYEYpZoURMg%3D%3D&position=23&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Walgreens Boots Alliance', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'United States', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree and at least 2 years of experience in data engineering; OR Graduate Degree in a technical discipline\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=aFcZ88nEiCcHRqBjOx4uqw%3D%3D&trackingId=70skYnyoHnyfClky50Zqkg%3D%3D&position=7&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Robinhood', 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': 'Menlo Park, CA; New York, NY; Seattle, WA; Washington, DC', 'salary': '$157000 - $185000', 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Not specified', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nrobinhood-3737638667?refId=aFcZ88nEiCcHRqBjOx4uqw%3D%3D&trackingId=yMRQG0%2FEWArUbngnOgEf9Q%3D%3D&position=14&pageNum=0'}, {'title': 'Data Engineer', 'company': 'KPIT', 'description': 'We KPIT (www.kpit.com) are a global technologies company specializing in CASE (Connected, Autonomous, Shared, Electric)\\ndomains. Systems and Software in Electric & Conventional Powertrain, Autonomous Driving & ADAS, Digital Connected\\nSolutions, Connected Vehicles, Vehicle Diagnostics, and Vehicle Networks.Join the leading software development and\\nintegration team helping mobility leapfrog towards a clean, smart, and safe future. A company specializing in embedded\\nsoftware, AI, and digital solutions, KPIT accelerates clients’ implementation of next-generation\\ntechnologies.Responsibilities:Develop and maintain ETL (Extract, Transform, Load) processes for efficient data\\nintegration.Able to design and implement database schemas and write queries to retrieve and manipulate data.Build and\\noptimize efficient data pipelines and architectures.Ensure data availability, integrity, and security.Using scripting\\nlanguages (e.g., Python, Bash) for automation of data workflows and processes.Implementing containerization (e.g.,\\nDocker) and orchestration tools (e.g., Kubernetes) for efficient deployment and management of data\\napplications.Collaborate with data scientists, analysts, and other stakeholders to understand data\\nrequirements.Requirements:Bachelor’s/master’s degree.Experience in data engineer role.Proficiency in designing and\\nimplementing ETL processes to move and transform data (e.g., extracting data, data cleaning, joining data).Proficient in\\nat least one programming language, such as Python, Java, or Scala.Proficiency in using SQL for data querying and\\nmanipulation.Good understanding of database systems, both relational and NoSQL.Good understanding of data warehousing\\nand big data technologies, such as Hadoop, Spark, and Kafka, Amazon Redshift, Google BigQuery.Knowledge on Apache Spark\\nand PySpark.Familiarity with AWS/Azure/GCP Cloud platform.Familiar with concepts such as scalability, fault tolerance,\\nand load balancing.Good understanding of continuous integration/continuous delivery (CI/CD).Knowledge of\\ncontainerization (e.g., Docker) and orchestration tools (e.g., Kubernetes).', 'location': 'Global', 'salary': None, 'job_function': 'Engineering', 'industries': 'Motor Vehicle Manufacturing', 'employment_type': 'Full-time', 'seniority_level': 'Associate', 'education_level': 'Bachelor’s/master’s degree', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nkpit-3763812955?refId=b6qNTI3myDV%2FxeyC4CXYqQ%3D%3D&trackingId=zkQlez8ughzmf5HgswuhNw%3D%3D&position=24&pageNum=0'}, {'title': None, 'company': None, 'description': \"Data EngineerSierra has been asked by a large risk management and intelligence services firm to identify a Data Engineer\\nto join their staff. You'll play a critical role in designing, building, and maintaining thier dark web and open-source\\ndata repositories, search applications, and APIs. Will also work closely with a cross-functional team to improve and\\nmaintain the reliability, scalability, and performance of these tools and technologies.PAY: 150K plus 10% bonus, great\\nbenefits. No sponsorship available.LOCATION: Hybrid remote/in-office in downtown Chicago (2-3 dayds per week). Chicago\\narea applicants only - no third parties please.ResponsibilitiesDesign and develop scalable search applications to enable\\nefficient data retrieval and indexing from the deep and dark web.Work with internal and external stakeholders to\\noptimize data infrastructure and identify cost savings, where possible.Build and maintain data pipelines to ingest,\\ntransform, and process large volumes of open, deep, and dark web data from diverse sources.Develop and maintain API\\nendpoints for querying dark web data, ensuring efficient and reliable access to our systems.Monitor and optimize search\\nperformance, address bottlenecks, and implement enhancementsImplement data quality and validation measures to ensure\\naccuracy and integrity of indexed data.Identify and integrate optimal database solutions.QualificationsExpertise in data\\nmodeling, infrastructure design, and data integration to enhance thier dataBachelor's or Master's degree in Computer\\nScience, Data Science, or a related field.Proficiency in working with large-scale data processing frameworks, especially\\nElasticsearchSolid understanding of data modeling and schema design principles for efficient search and\\nretrievalFamiliarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or\\nprocessesExperience with API development and management, including authentication, versioning, and performance\\noptimizationProven experience as a Data Engineer, preferably in the development and management of search engines and\\nAPIsDemonstrated knowledge of cloud platforms, especially AWS and AzureExcellent communication and collaboration skills\\nto work effectively in a cross-functional team environmentFamiliarity with building CI/CD pipelines to ensure the\\ndelivery of high-quality softwareKnowledge of data privacy and security considerations when working with sensitive\\ndataStrong programming skills in Python\", 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'Information Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-chicago-165k-at-sierra-\\nits-3676932344?refId=H4EGkXNa5BoDzVIjuvwgQQ%3D%3D&trackingId=X3ida1yMn1m3eaHyyV5d%2Fw%3D%3D&position=9&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Robinhood', 'description': \"Join a leading fintech company that’s democratizing finance for all.Robinhood was founded on a simple idea: that our\\nfinancial markets should be accessible to all. With customers at the heart of our decisions, Robinhood is lowering\\nbarriers and providing greater access to financial information. Together, we are building products and services that\\nhelp create a financial system everyone can participate in.As we continue to build...We’re seeking curious, growth\\nminded thinkers to help shape our vision, structures and systems; playing a key-role as we launch into our ambitious\\nfuture. If you’re invigorated by our mission, values, and drive to change the world — we’d love to have you apply.About\\nthe team + role Robinhood is a metrics driven company and data is foundational to all key decisions from growth strategy\\nto product optimization to our day-to-day operations. We are looking for a Data Engineer to build and maintain\\nfoundational datasets that will allow us to reliably and efficiently power decision making at Robinhood. These datasets\\ninclude application events, database snapshots, and the derived datasets that describe and track Robinhood's key metrics\\nacross all products. You’ll partner closely with engineers, data scientists and business teams to power analytics,\\nexperimentation, and machine learning use cases. We are a fast-paced team in a fast growing company and this is a unique\\nopportunity to help lay the foundation for reliable, impactful, data-driven decisions across the company for years to\\ncome.The role is located in the office location(s) listed on this job description which will align with our in-office\\nworking environment. Please connect with your recruiter for more information regarding our in-office philosophy and\\nexpectations.What You’ll DoHelp define and build key datasets across all Robinhood product areas. Lead the evolution of\\nthese datasets as use cases grow.Build scalable data pipelines using Python, Spark and Airflow to move data from\\ndifferent applications into our data lake.Partner with upstream engineering teams to enhance data generation\\npatterns.Partner with data consumers across Robinhood to understand consumption patterns and design intuitive data\\nmodels.Ideate and contribute to shared data engineering tooling and standards.Define and promote data engineering best\\npractices across the company.What You Bring4+ years of professional experience building end-to-end data pipelinesProven\\nability to implement software engineering-caliber code (preferably Python)Expert at building and maintaining large-scale\\ndata pipelines using open source frameworks (Spark, Flink, etc)Strong SQL (Presto, Spark SQL, etc) skills.Experience\\nsolving problems across the data stack (Data Infrastructure, Analytics and Visualization platforms)Expert collaborator\\nwith the ability to democratize data through actionable insights and solutions.What We OfferMarket competitive and pay\\nequity-focused compensation structure100% paid health insurance for employees with 90% coverage for dependentsAnnual\\nlifestyle wallet for personal wellness, learning and development, and more!Lifetime maximum benefit for family forming\\nand fertility benefitsDedicated mental health support for employees and eligible dependentsGenerous time away including\\ncompany holidays, paid time off, sick time, parental leave, and more!Lively office environment with catered meals, fully\\nstocked kitchens, and geo-specific commuter benefitsBase pay for the successful applicant will depend on a variety of\\njob-related factors, which may include education, training, experience, location, business needs, or market demands. The\\nexpected salary range for this role is based on the location where the work will be performed and is aligned to one of 3\\ncompensation zones. This role is also eligible to participate in a Robinhood bonus plan and Robinhood’s equity plan. For\\nother locations not listed, compensation can be discussed with your recruiter during the interview process.US Zone 1:\\n$157000 - $185000Menlo Park, CA; New York, NY; Seattle, WA; Washington, DCUS Zone 2: $139000 - $163000Denver, CO;\\nWestlake, TX; Chicago, ILUS Zone 3: $122000 - $144000Lake Mary, FLClick Here To Learn More About Robinhood’s\\nBenefits.We’re looking for more growth-minded and collaborative people to be a part of our journey in democratizing\\nfinance for all. If you’re ready to give 100% in helping us achieve our mission—we’d love to have you apply even if you\\nfeel unsure about whether you meet every single requirement in this posting. At Robinhood, we're looking for people\\ninvigorated by our mission, values, and drive to change the world, not just those who simply check off all the\\nboxes.Robinhood embraces a diversity of backgrounds and experiences and provides equal opportunity for all applicants\\nand employees. We are dedicated to building a company that represents a variety of backgrounds, perspectives, and\\nskills. We believe that the more inclusive we are, the better our work (and work environment) will be for everyone.\\nAdditionally, Robinhood provides reasonable accommodations for candidates on request and respects applicants' privacy\\nrights. To review Robinhood's Privacy Policy please review the specific policy applicable to your region: Canada\\nApplicant Privacy Policy / UK/EEA Applicant Privacy Policy / US Applicant Privacy Policy\", 'location': 'Menlo Park, CA; New York, NY; Seattle, WA; Washington, DC', 'salary': '$157000 - $185000', 'job_function': 'Information Technology', 'industries': 'Financial Services', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Not specified', 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-\\nrobinhood-3737638667?refId=1j12dYfGDaJUMcjoDEzPUg%3D%3D&trackingId=48le4rYYpSeUp36tCgUvag%3D%3D&position=14&pageNum=0'}, {'title': None, 'company': None, 'description': 'RemoteData Engineer12+ MonthsDevelop and maintain SQL scripts, stored procedures, and queries to extract and manipulate\\ndata from various sources, including MSSQL and Postgres databases.Work with MuleSoft (Mule) for data integration and API\\ndevelopment, if required or willing to learn it.Monitor and optimize data pipelines, troubleshoot issues, and ensure\\ndata integrity and performance.Collaborate with data analysts to understand data requirements and support their data\\nanalysis and modeling efforts.Stay up to date with industry trends, best practices, and emerging technologies related to\\ndata engineering, Snowflake, ETL, SQL, and related tools.Solid understanding and hands-on experience with ETL tools and\\nframeworks.Excellent communication and collaboration skills to work effectively with cross-functional teams.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'IT Services and IT Consulting', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-accroid-\\ninc-3778553874?refId=aFcZ88nEiCcHRqBjOx4uqw%3D%3D&trackingId=1RxhaiMT1K42HsWKAmuF8A%3D%3D&position=18&pageNum=0'}, {'title': None, 'company': None, 'description': 'Role: Data Engineer (DE)Location: Scottsdale AZ (day 1 onsite)Duration: Fulltime Must have skill set: Java, Scala,\\nPython, Spark, S3, Glue, RedshiftYou have 6-8 years of relevant software development experience. You have hands-on\\nexperience in Java/Scala/Python, Spark, S3, Glue, Redshift. This is critical. Highly analytical and data oriented.\\nExperience in SQL, NoSql Database Data masking of on prem PII data. Develop API calls with using secure data transfer.\\nTake standard output data to lower environments for pre prod testing! Enable secured channels for data models and data\\nscience activities. Need a solution that can scale to handle millions of data. i.e., masking 10 million records in 15\\nmins You have experience with development tools and agile methodologies.', 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'Human Resources Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-de-at-zortech-\\nsolutions-3778562998?refId=WmSwrt9r4vRYmFNoHaH%2B9w%3D%3D&trackingId=X6qJV7owOU6fzGrbdrZCfA%3D%3D&position=14&pageNum=0'}, {'title': 'Big Data Engineer', 'company': 'Walgreens', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'United States', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree\", 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=1j12dYfGDaJUMcjoDEzPUg%3D%3D&trackingId=44G8c4yT3ZC6EKGpcEKYMQ%3D%3D&position=6&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Dollar General Corporation', 'description': 'Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers\\nSave time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks,\\nhealth and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in\\nconvenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still\\ngrowing. Learn more about Dollar General at www.dollargeneral.com.General Summary Dollar General Corporation has been\\ndelivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by\\noffering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning\\nsupplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations.\\nDollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General\\nat www.dollargeneral.com.Duties & Responsibilities Advanced working SQL knowledge and experience working with relational\\ndatabases, query authoring (SQL) as well as working familiarity with a variety of databases.Assemble large, complex data\\nsets that meet functional / non-functional business requirements.Identify, design, and implement internal process\\nimprovements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability,\\netc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of\\ndata sources using SQL and cloud technologies.Build analytics tools that utilize the data pipelines to provide\\nactionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work\\nwith stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support\\ntheir data infrastructure needs.Knowledge, Skills and Abilities Knowledge of programming languages (e.g. Java and\\nPython)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT,\\nor similar field; a Master’s is a plusData engineering certification (e.g IBM Certified Data Engineer) is a\\nplusExperience with big data tools Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases,\\nincluding Postgres and Cassandra.Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow,\\netc.Experience with Snowflake/Azure cloud services EC2, EMR, RDS, RedshiftExperience with stream-processing systems\\nStorm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages Python, Java, C++,\\nScala, etcWork Experience &/or Education Degree in information technology or computer science with additional vendor-\\nspecific certification.BS or MS degree in Computer Science or a related technical field4+ years of Python or Java\\ndevelopment experience4+ years of SQL experience (No-SQL experience is a plus)4+ years of experience with schema design\\nand dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience\\ndesigning, building, and maintaining data processing systemsExperience working with a cloud platform such as Snowflake /\\nAzure or Databricks#mogul#', 'location': '47 states', 'salary': None, 'job_function': 'Information Technology', 'industries': 'Retail', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'Degree in Computer Science, IT, or similar field; a Master’s is a plus', 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-dollar-\\ngeneral-3745487485?refId=b6qNTI3myDV%2FxeyC4CXYqQ%3D%3D&trackingId=5P1KGcROKZ8q%2FeFwlbfevg%3D%3D&position=22&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Dollar General Corporation', 'description': 'Dollar General Corporation has been delivering value to shoppers for more than 80 years. Dollar General helps shoppers\\nSave time. Save money. Every day.® by offering products that are frequently used and replenished, such as food, snacks,\\nhealth and beauty aids, cleaning supplies, basic apparel, housewares and seasonal items at everyday low prices in\\nconvenient neighborhood locations. Dollar General operates more than 18,000 stores in 47 states, and we’re still\\ngrowing. Learn more about Dollar General at www.dollargeneral.com.General Summary Dollar General Corporation has been\\ndelivering value to shoppers for more than 80 years. Dollar General helps shoppers Save time. Save money. Every day.® by\\noffering products that are frequently used and replenished, such as food, snacks, health and beauty aids, cleaning\\nsupplies, basic apparel, housewares and seasonal items at everyday low prices in convenient neighborhood locations.\\nDollar General operates more than 18,000 stores in 47 states, and we’re still growing. Learn more about Dollar General\\nat www.dollargeneral.com.Duties & Responsibilities Advanced working SQL knowledge and experience working with relational\\ndatabases, query authoring (SQL) as well as working familiarity with a variety of databases.Assemble large, complex data\\nsets that meet functional / non-functional business requirements.Identify, design, and implement internal process\\nimprovements automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability,\\netc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of\\ndata sources using SQL and cloud technologies.Build analytics tools that utilize the data pipelines to provide\\nactionable insights into customer acquisition, operational efficiency and other key business performance metrics.Work\\nwith stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support\\ntheir data infrastructure needs.Knowledge, Skills and Abilities Knowledge of programming languages (e.g. Java and\\nPython)Hands-on experience with SQL database designGreat numerical and analytical skillsDegree in Computer Science, IT,\\nor similar field; a Master’s is a plusData engineering certification (e.g IBM Certified Data Engineer) is a\\nplusExperience with big data tools Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases,\\nincluding Postgres and Cassandra.Experience with data pipeline and workflow management tools Azkaban, Luigi, Airflow,\\netc.Experience with Snowflake/Azure cloud services EC2, EMR, RDS, RedshiftExperience with stream-processing systems\\nStorm, Spark-Streaming, etc.Experience with object-oriented/object function scripting languages Python, Java, C++,\\nScala, etcWork Experience &/or Education Degree in information technology or computer science with additional vendor-\\nspecific certification.BS or MS degree in Computer Science or a related technical field4+ years of Python or Java\\ndevelopment experience4+ years of SQL experience (No-SQL experience is a plus)4+ years of experience with schema design\\nand dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience\\ndesigning, building, and maintaining data processing systemsExperience working with a cloud platform such as Snowflake /\\nAzure or Databricks#mogul#', 'location': 'Unknown', 'salary': '', 'job_function': 'Information Technology', 'industries': 'Retail', 'employment_type': 'Full-time', 'seniority_level': 'Entry level', 'education_level': 'BS or MS degree in Computer Science or a related technical field', 'notes': '', 'hiring_manager': 'Unknown', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-dollar-\\ngeneral-3745487485?refId=hUqfFzzrGisp%2F7qdM%2BRWqw%3D%3D&trackingId=EDnB5bsHYH8wOkto1smLPw%3D%3D&position=23&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Remote', 'description': 'RemoteData Engineer12+ MonthsDevelop and maintain SQL scripts, stored procedures, and queries to extract and manipulate\\ndata from various sources, including MSSQL and Postgres databases.Work with MuleSoft (Mule) for data integration and API\\ndevelopment, if required or willing to learn it.Monitor and optimize data pipelines, troubleshoot issues, and ensure\\ndata integrity and performance.Collaborate with data analysts to understand data requirements and support their data\\nanalysis and modeling efforts.Stay up to date with industry trends, best practices, and emerging technologies related to\\ndata engineering, Snowflake, ETL, SQL, and related tools.Solid understanding and hands-on experience with ETL tools and\\nframeworks.Excellent communication and collaboration skills to work effectively with cross-functional teams.', 'location': 'N/A', 'salary': 'N/A', 'job_function': 'Information Technology', 'industries': 'IT Services and IT Consulting', 'employment_type': 'Contract', 'seniority_level': 'Mid-Senior level', 'education_level': 'N/A', 'notes': '', 'hiring_manager': 'N/A', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-at-accroid-\\ninc-3778553874?refId=1j12dYfGDaJUMcjoDEzPUg%3D%3D&trackingId=SppvIQdh651pbFpJ9dN%2Baw%3D%3D&position=19&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Large risk management and intelligence services firm', 'description': \"Data EngineerSierra has been asked by a large risk management and intelligence services firm to identify a Data Engineer\\nto join their staff. You'll play a critical role in designing, building, and maintaining thier dark web and open-source\\ndata repositories, search applications, and APIs. Will also work closely with a cross-functional team to improve and\\nmaintain the reliability, scalability, and performance of these tools and technologies.PAY: 150K plus 10% bonus, great\\nbenefits. No sponsorship available.LOCATION: Hybrid remote/in-office in downtown Chicago (2-3 dayds per week). Chicago\\narea applicants only - no third parties please.ResponsibilitiesDesign and develop scalable search applications to enable\\nefficient data retrieval and indexing from the deep and dark web.Work with internal and external stakeholders to\\noptimize data infrastructure and identify cost savings, where possible.Build and maintain data pipelines to ingest,\\ntransform, and process large volumes of open, deep, and dark web data from diverse sources.Develop and maintain API\\nendpoints for querying dark web data, ensuring efficient and reliable access to our systems.Monitor and optimize search\\nperformance, address bottlenecks, and implement enhancementsImplement data quality and validation measures to ensure\\naccuracy and integrity of indexed data.Identify and integrate optimal database solutions.QualificationsExpertise in data\\nmodeling, infrastructure design, and data integration to enhance thier dataBachelor's or Master's degree in Computer\\nScience, Data Science, or a related field.Proficiency in working with large-scale data processing frameworks, especially\\nElasticsearchSolid understanding of data modeling and schema design principles for efficient search and\\nretrievalFamiliarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or\\nprocessesExperience with API development and management, including authentication, versioning, and performance\\noptimizationProven experience as a Data Engineer, preferably in the development and management of search engines and\\nAPIsDemonstrated knowledge of cloud platforms, especially AWS and AzureExcellent communication and collaboration skills\\nto work effectively in a cross-functional team environmentFamiliarity with building CI/CD pipelines to ensure the\\ndelivery of high-quality softwareKnowledge of data privacy and security considerations when working with sensitive\\ndataStrong programming skills in Python\", 'location': 'Hybrid remote/in-office in downtown Chicago (2-3 days per week)', 'salary': '150K plus 10% bonus', 'job_function': 'Information Technology', 'industries': 'Information Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': \"Bachelor's or Master's degree in Computer Science, Data Science, or a related field\", 'notes': '', 'hiring_manager': 'Sierra', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-chicago-165k-at-sierra-\\nits-3676932344?refId=b6qNTI3myDV%2FxeyC4CXYqQ%3D%3D&trackingId=%2B42U4u%2BdpmKZbY7rPinjQg%3D%3D&position=9&pageNum=0'}, {'title': 'Big Data Engineer', 'company': 'Walgreens', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'United States', 'salary': 'Not specified', 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree and at least 2 years of experience in data engineering; OR Graduate Degree in a technical discipline\", 'notes': '', 'hiring_manager': 'Not specified', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=H4EGkXNa5BoDzVIjuvwgQQ%3D%3D&trackingId=ezN2KRZT9QclvcgbDIcQDg%3D%3D&position=7&pageNum=0'}, {'title': None, 'company': None, 'description': \"Data EngineerSierra has been asked by a large risk management and intelligence services firm to identify a Data Engineer\\nto join their staff. You'll play a critical role in designing, building, and maintaining thier dark web and open-source\\ndata repositories, search applications, and APIs. Will also work closely with a cross-functional team to improve and\\nmaintain the reliability, scalability, and performance of these tools and technologies.PAY: 150K plus 10% bonus, great\\nbenefits. No sponsorship available.LOCATION: Hybrid remote/in-office in downtown Chicago (2-3 dayds per week). Chicago\\narea applicants only - no third parties please.ResponsibilitiesDesign and develop scalable search applications to enable\\nefficient data retrieval and indexing from the deep and dark web.Work with internal and external stakeholders to\\noptimize data infrastructure and identify cost savings, where possible.Build and maintain data pipelines to ingest,\\ntransform, and process large volumes of open, deep, and dark web data from diverse sources.Develop and maintain API\\nendpoints for querying dark web data, ensuring efficient and reliable access to our systems.Monitor and optimize search\\nperformance, address bottlenecks, and implement enhancementsImplement data quality and validation measures to ensure\\naccuracy and integrity of indexed data.Identify and integrate optimal database solutions.QualificationsExpertise in data\\nmodeling, infrastructure design, and data integration to enhance thier dataBachelor's or Master's degree in Computer\\nScience, Data Science, or a related field.Proficiency in working with large-scale data processing frameworks, especially\\nElasticsearchSolid understanding of data modeling and schema design principles for efficient search and\\nretrievalFamiliarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or\\nprocessesExperience with API development and management, including authentication, versioning, and performance\\noptimizationProven experience as a Data Engineer, preferably in the development and management of search engines and\\nAPIsDemonstrated knowledge of cloud platforms, especially AWS and AzureExcellent communication and collaboration skills\\nto work effectively in a cross-functional team environmentFamiliarity with building CI/CD pipelines to ensure the\\ndelivery of high-quality softwareKnowledge of data privacy and security considerations when working with sensitive\\ndataStrong programming skills in Python\", 'location': None, 'salary': None, 'job_function': 'Information Technology', 'industries': 'Information Services', 'employment_type': 'Full-time', 'seniority_level': 'Mid-Senior level', 'education_level': None, 'notes': '', 'hiring_manager': None, 'url': 'https://www.linkedin.com/jobs/view/data-engineer-chicago-165k-at-sierra-\\nits-3676932344?refId=yYkaI0G4nwepPxFJ3klj8A%3D%3D&trackingId=NT%2BfDfWBbmZyxzADGU5gOA%3D%3D&position=9&pageNum=0'}, {'title': 'Data Engineer I', 'company': 'N/A', 'description': 'Title:Data Engineer I, Req# 27194964Location:Cupertino,CA(Hybrid)Contract: 12+ MonthJob Description Develop data\\nautomation tool for collection, processing, and storing lab data. Set up, maintain, and monitor continuous operation\\nclient devices in labs. Develop test scripts for various client devices. Maintain software revisions through GitHub.\\nBuild ETL for telemetry field dataset and automate data integrity and optimization routines for automatic reporting,\\nanalysis, and error detection. Analyze user and experimental data and use engineering and analytical understanding to\\nresolve battery problems. Provide ad-hoc analysis as necessary.Qualifications 1+ years of experience in software\\nengineering/data science engineering. Proficient in Python for data processing and analysis tool development. Proficient\\nin Linux/Unix (Bash and Shell). Proficient in C for software development Proficient in revision control software such as\\nGitHub. Strong working knowledge in designing, building, and maintaining data ETL pipeline. Experience in SQL.\\nExperience in database modeling and data warehousing principles. Familiarity with job scheduling system. Experience in\\ndata science and analytics, statistical analyses, A/B testing and conducting experiments and investigations in large-\\nscale usage data environment. Self-started with a proven ability to handle multiple tasks with strict deadlines. Proven\\ncreativity to go beyond current tools to deliver best solution to the problem. Outstanding problem solving, critical\\nthinking and interpersonal skills.PayRate: $50-55/hr, W2', 'location': 'Cupertino, CA (Hybrid)', 'salary': '$50-55/hr, W2', 'job_function': 'Information Technology', 'industries': 'Staffing and Recruiting', 'employment_type': 'Contract', 'seniority_level': 'Entry level', 'education_level': 'N/A', 'notes': '', 'hiring_manager': 'N/A', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-i-at-\\nwinmax-3759969537?refId=hUqfFzzrGisp%2F7qdM%2BRWqw%3D%3D&trackingId=fc7ljrtnNK8OoFCegA2m%2BA%3D%3D&position=16&pageNum=0'}, {'title': 'Data Engineer', 'company': 'Walgreens', 'description': \"Job SummaryBuilds & maintains big data pipelines to support advanced analytics and data science solutions. Identifies\\nvaluable internal and external data. Collaborates closely with data scientists to define data for the design,\\ndevelopment, and deployment of new solutions that support business priorities.Job Responsibilities Develops software\\nthat processes, stores and serves data for use by others.Develops data structures and pipelines to organize, collect and\\nstandardize data that helps generate insights and addresses reporting needs.Writes ETL (Extract / Transform / Load)\\nprocesses, designs database systems and develops tools for real-time and offline analytic processing.Develops data\\npipelines that are scalable, repeatable and secure.Troubleshoots software and processes for data consistency and\\nintegrity. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility\\nstandards.Effectively resolves problems and roadblocks as they occur.Interacts with internal and external peers and/or\\nmanagers to exchange semi-complex information related to assigned activities.About Walgreens and WBAWalgreens\\n(www.walgreens.com) is included in the U.S. Retail Pharmacy and U.S. Healthcare segments of Walgreens Boots Alliance,\\nInc. (Nasdaq: WBA), an integrated healthcare, pharmacy and retail leader with a 170 year heritage of caring for\\ncommunities. WBA’s purpose is to create more joyful lives through better health. Operating nearly 9,000 retail locations\\nacross America, Puerto Rico and the U.S. Virgin Islands, Walgreens is proud to be a neighborhood health destination\\nserving nearly 10 million customers each day. Walgreens pharmacists play a critical role in the U.S. healthcare system\\nby providing a wide range of pharmacy and healthcare services, including those that drive equitable access to care for\\nthe nation’s medically underserved populations. To best meet the needs of customers and patients, Walgreens offers a\\ntrue omnichannel experience, with fully integrated physical and digital platforms supported by the latest technology to\\ndeliver high quality products and services in communities nationwide.Basic QualificationsBachelor's degree and at least\\n2 years of experience in data engineering; OR Graduate Degree in a technical discipline.Advanced knowledge of\\nSQLExperience establishing and maintaining key relationships with internal (peers, business partners and leadership) and\\nexternal (business community, clients and vendors) within a matrix organization to ensure quality standards for\\nservice.Experience analyzing and reporting data in order to identify issues, trends, or exceptions to drive improvement\\nof results and find solutions.Willing to travel up to 10% of the time for business purposes (within state and out of\\nstate).Preferred QualificationsGraduate Degree in a technical disciplineExperience with REST API developmentExperience\\nwith Azure application deploymentExperience in Azure technologies like Azure Data Factory, Azure Databricks using Python\\nor Scala, App Services, Azure Data Lake, Azure Functions, Event Hubs, Event Grids and Logic AppExperience in building\\nAzure DevOPS pipelines to enable CI/CD, Infrastructure as Code (Iaas), and automation.\", 'location': 'Undefined', 'salary': 'Not specified', 'job_function': 'Information Technology', 'industries': 'Wellness and Fitness Services, Pharmaceutical Manufacturing, and Retail', 'employment_type': 'Full-time', 'seniority_level': 'Not Applicable', 'education_level': \"Bachelor's degree, Graduate Degree in a technical discipline\", 'notes': '', 'hiring_manager': 'Undefined', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-talend-at-\\nwalgreens-3763922227?refId=HSbYYR67Af9HPAndWNdraw%3D%3D&trackingId=rrondlgUtZg1nVDt2BGMQw%3D%3D&position=6&pageNum=0'}, {'title': 'Data Engineer I', 'company': 'Unknown', 'description': 'Title:Data Engineer I, Req# 27194964Location:Cupertino,CA(Hybrid)Contract: 12+ MonthJob Description Develop data\\nautomation tool for collection, processing, and storing lab data. Set up, maintain, and monitor continuous operation\\nclient devices in labs. Develop test scripts for various client devices. Maintain software revisions through GitHub.\\nBuild ETL for telemetry field dataset and automate data integrity and optimization routines for automatic reporting,\\nanalysis, and error detection. Analyze user and experimental data and use engineering and analytical understanding to\\nresolve battery problems. Provide ad-hoc analysis as necessary.Qualifications 1+ years of experience in software\\nengineering/data science engineering. Proficient in Python for data processing and analysis tool development. Proficient\\nin Linux/Unix (Bash and Shell). Proficient in C for software development Proficient in revision control software such as\\nGitHub. Strong working knowledge in designing, building, and maintaining data ETL pipeline. Experience in SQL.\\nExperience in database modeling and data warehousing principles. Familiarity with job scheduling system. Experience in\\ndata science and analytics, statistical analyses, A/B testing and conducting experiments and investigations in large-\\nscale usage data environment. Self-started with a proven ability to handle multiple tasks with strict deadlines. Proven\\ncreativity to go beyond current tools to deliver best solution to the problem. Outstanding problem solving, critical\\nthinking and interpersonal skills.PayRate: $50-55/hr, W2', 'location': 'Cupertino, CA (Hybrid)', 'salary': '$50-55/hr, W2', 'job_function': 'Information Technology', 'industries': 'Staffing and Recruiting', 'employment_type': 'Contract', 'seniority_level': 'Entry level', 'education_level': 'Unknown', 'notes': '', 'hiring_manager': 'Unknown', 'url': 'https://www.linkedin.com/jobs/view/data-engineer-i-at-\\nwinmax-3759969537?refId=ACNf%2FwdQfBNm%2BGsMg51brQ%3D%3D&trackingId=fh0BRiq22lWMZNjIne9stA%3D%3D&position=16&pageNum=0'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lead_docs_dir = Path(conf.settings.PUBLIC_ASSETS_DIR) / 'seeds' / 'leads'\n",
    "leads_json_schema = schemas.LeadCreate.model_json_schema()\n",
    "\n",
    "with open(lead_docs_dir / 'leads.json', 'r') as lead_docs_json:\n",
    "    lead_docs = json.load(lead_docs_json)\n",
    "\n",
    "\n",
    "nl_query_engine = JSONQueryEngine(\n",
    "    json_value=lead_docs,\n",
    "    json_schema=leads_json_schema,\n",
    "    llm=llm,\n",
    ")\n",
    "raw_query_engine = JSONQueryEngine(\n",
    "    json_value=lead_docs,\n",
    "    json_schema=leads_json_schema,\n",
    "    llm=llm,\n",
    "    synthesize_response=False,\n",
    ")\n",
    "\n",
    "print(lead_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.5590165776340315 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-R8bbBzuAHajq3HRWLuzeOXGa on tokens per min (TPM): Limit 10000, Requested 138893. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.30920745398890914 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-R8bbBzuAHajq3HRWLuzeOXGa on tokens per min (TPM): Limit 10000, Requested 138893. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.554950795877855 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-R8bbBzuAHajq3HRWLuzeOXGa on tokens per min (TPM): Limit 10000, Requested 138893. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 7.002800925232774 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-R8bbBzuAHajq3HRWLuzeOXGa on tokens per min (TPM): Limit 10000, Requested 138893. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 14.212032201025844 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-R8bbBzuAHajq3HRWLuzeOXGa on tokens per min (TPM): Limit 10000, Requested 138893. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-R8bbBzuAHajq3HRWLuzeOXGa on tokens per min (TPM): Limit 10000, Requested 138893. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the first 5 items\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m ]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m----> 7\u001b[0m     nl_response \u001b[38;5;241m=\u001b[39m \u001b[43mnl_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m       \u001b[49m\u001b[43mmsg\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m raw_query_engine\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m     11\u001b[0m         msg\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     display(\n\u001b[1;32m     15\u001b[0m         Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<h1>Natural language Response</h1><br><b>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnl_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</b>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:146\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_drop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:98\u001b[0m, in \u001b[0;36mDispatcher.span_drop\u001b[0;34m(self, id_, bound_args, instance, err, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m c:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m c\u001b[38;5;241m.\u001b[39mspan_handlers:\n\u001b[0;32m---> 98\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_drop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mid_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbound_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m c\u001b[38;5;241m.\u001b[39mpropagate:\n\u001b[1;32m    107\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/span_handlers/base.py:77\u001b[0m, in \u001b[0;36mBaseSpanHandler.span_drop\u001b[0;34m(self, id_, bound_args, instance, err, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspan_drop\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     75\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Logic for dropping a span i.e. early exit.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     span \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_to_drop_span\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m span:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_span_id \u001b[38;5;241m==\u001b[39m id_:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/span_handlers/null.py:71\u001b[0m, in \u001b[0;36mNullSpanHandler.prepare_to_drop_span\u001b[0;34m(self, id_, bound_args, instance, err, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Logic for droppping a span.\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:144\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_drop(id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, err\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py:51\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     50\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 51\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(QueryEndEvent())\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/indices/struct_store/json_query.py:198\u001b[0m, in \u001b[0;36mJSONQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    195\u001b[0m     print_text(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> JSONPath Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_path_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_synthesize_response:\n\u001b[0;32m--> 198\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_synthesis_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_json_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_path_response_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson_path_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_path_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(json_path_output)\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:146\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_drop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:98\u001b[0m, in \u001b[0;36mDispatcher.span_drop\u001b[0;34m(self, id_, bound_args, instance, err, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m c:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m c\u001b[38;5;241m.\u001b[39mspan_handlers:\n\u001b[0;32m---> 98\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_drop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mid_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbound_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m            \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m c\u001b[38;5;241m.\u001b[39mpropagate:\n\u001b[1;32m    107\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/span_handlers/base.py:77\u001b[0m, in \u001b[0;36mBaseSpanHandler.span_drop\u001b[0;34m(self, id_, bound_args, instance, err, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspan_drop\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     75\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Logic for dropping a span i.e. early exit.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     span \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_to_drop_span\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbound_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m span:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_span_id \u001b[38;5;241m==\u001b[39m id_:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/span_handlers/null.py:71\u001b[0m, in \u001b[0;36mNullSpanHandler.prepare_to_drop_span\u001b[0;34m(self, id_, bound_args, instance, err, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Logic for droppping a span.\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:144\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_drop(id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, err\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/llms/llm.py:413\u001b[0m, in \u001b[0;36mLLM.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[1;32m    412\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[0;32m--> 413\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:130\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    116\u001b[0m     LLMChatStartEvent(\n\u001b[1;32m    117\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39m_self\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    123\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    124\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     },\n\u001b[1;32m    129\u001b[0m )\n\u001b[0;32m--> 130\u001b[0m f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py:296\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/llama_index/llms/openai/base.py:362\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[1;32m    361\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 362\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m openai_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    368\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:1213\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1201\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1210\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1211\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:902\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    895\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    901\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    977\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    977\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:978\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    977\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:1026\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/baldin/backend/.venv/lib/python3.11/site-packages/openai/_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    990\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    992\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    996\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    997\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1001\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-R8bbBzuAHajq3HRWLuzeOXGa on tokens per min (TPM): Limit 10000, Requested 138893. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    \"Return the first 5 items\",\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "\n",
    "    nl_response = nl_query_engine.query(\n",
    "       msg\n",
    "    )\n",
    "    raw_response = raw_query_engine.query(\n",
    "        msg\n",
    "    )\n",
    "\n",
    "    display(\n",
    "        Markdown(f\"<h1>Natural language Response</h1><br><b>{nl_response}</b>\")\n",
    "    )\n",
    "    display(Markdown(f\"<h1>Raw JSON Response</h1><br><b>{raw_response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
